\section{Nonlinear Optimization}\label{d1c9db1}

\def\pd{\succ} % positive definite
\def\psd{\succeq} % positive semidefinite

\Definition{1.1.2}{The argmin set}\label{b27e478}

The set which minimizes values of $f$ over a domain $X$ is denoted by
$$
	\argmin_{x\in X}f:=\Set{x\in X}{f(x)=\inf_Xf}
$$

\Definition{1.1.3}{Local vs. global minima}\label{bc7900e}

Let $X\subset\R^n$ and $f:\R^n\to\R$. Then $\bar x\in X$ is called a
\begin{itemize}
	\item global minimizer of $f$ over $X$ if $\bar x\in\argmin_Xf$,
	      i.e. $\forall x\in X:f(\bar x)\leq f(x)$
	\item local minimizer of $f$ over $X$ if $\exists\epsilon>0$ such
	      that $\forall x\in X\cap B_\epsilon(\bar x):f(\bar x)\leq f(x)$
\end{itemize}
For strict global/local minimizers, the above conditions hold with
strict inequality.

\Definition{1.1.4}{Level sets and Lower level sets}\label{d6589cc}

For $f:\R^n\to\R$ the level set for the level $c\in R$ is given by
$$
	f^{-1}(\{c\})=\Set{x\in\R^n}{f(x)=c}
$$

The lower level set (or \textit{sublevel} set) of $f$ to the level
$c\in\R$ is
$$
	\text{lev}_cf:=f^{-1}((-\infty,c])=\Set{x\in\R^n}{f(x)\leq c}
$$

\Proposition{1.1.5}{}\label{bd1eec0}

Let $f:\R^n\to\R$ be continuous. If $\exists c\in\R$ such that
$\text{lev}_cf$ is non-empty and bounded then $f$ takes its minimum
over $\R^n$.

\Definition{1.2.1}{Convex sets}\label{e012971}

A set $C\subset\R^n$ is called convex if

$$
	\lambda x+(1-\lambda)y\in C\quad
	\forall(x,y\in C,\ \lambda\in(0,1))
$$

or simply a set which contains all connecting lines of points from the
set.

\Definition{1.2.3}{Convex functions}\label{a114065}

Let $C\subset\R^n$ be convex. Let $\lambda\in(0,1)$ and $x,y\in C$ and
let

Then $f:C\to\R$ is said to be

\begin{itemize}
	\item convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$$
	\item strictly convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$$
	\item strongly convex on $C$ if $\exists\mu>0$ such that
	      $$f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)$$
\end{itemize}

\Example{1.2.5}{Convex functions}\label{f96c8ae}

\begin{enumerata}
	\item $\exp:\R\to\R$ and $-\log:(0,\infty)\to\R$ are convex.
	\item \textit{(Affine functions)} $f:\R^n\to\R^m$ of the form
	$$
		f(x)=Ax-b\with(A\in\R^{m\times n},b\in\R^m)
	$$
	is called \textbf{affine} (linear). All affine functions, hence
	all linear functions ($b=0$) $\R^n\to\R$ are convex.
	\item \textit{(Norms)} Any norm $\norm\cdot$ on $\R^n$ is convex.
\end{enumerata}

\Proposition{1.2.6}{Convexity preserving operations}\label{ddce2a7}

\begin{enumerate}
	\item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
	      convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
	      convex.
	\item (Composition with affine mapping) $f:\R^n\to\R$ be convex
	      and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\Definition{2.1.1}{Directional derivative}\label{37eb747}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable
at $\bar x\in\R^n$ in the direction $d\in\R^n$ if
$$
	\lim_{t\downarrow0}\frac{f(\bar x+td)-f(x)}t
$$

exists. This limit is denoted by $f'(x;d)$ and is called the
directional derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we
call it directionally differentiable.

\Lemma{2.1.2}{Directional derivative and gradient}\label{ed67d29}

Let $D\subset \R^n$ be open and $f:D\to\R$
differentiable at $x\in D$. Then $f$ is directionally differentiable
at $x$ with
$$
	f'(x;d)=\nabla f(x)^Td\quad\forall(d\in\R^n)
$$

Where $f'(x;d)$ is the \href{37eb747}{directional derivative} of
$f$ at $x$ in the direction $d$.

\Lemma{2.1.4}{Basic optimality condition}\label{b3b5e10}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local
minimizer of $f$ over $X$ and $f$ is \href{37eb747}{directionally
	differentiable} at $\bar x$ then
$$
	f'(x;d)\geq 0\quad\forall(d\in\R^n)
$$

\Theorem{2.1.5}{Fermat's rule}\label{dc165c9}

Let $X\subset\R^n$ be open and $f:X\to\R$ differentiable at $\bar x\in
	X$. If $\bar x$ is a local minimizer (or maximizer) of $f$ over $X$
then $\nabla f(\bar x)=0$.

\Theorem{2.1.6}{Second-order necessary condition}\label{ce5370d}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x$ is a local minimizer (maximizer) of $f$
over $X$ then $\nabla^2f(\bar x)$ is positive (negative) semidefinite.

\Lemma{2.1.7}{}\label{eaa0d24}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x\in\R^n$ is such that $\nabla^2f(\bar x)$
positive definite then $\exists\,\mu,\epsilon>0$ such that
$B_\epsilon(\bar x)\subset X$ and
$$
	d^T\nabla^2f(x)d\geq\mu\norm d^2_2\quad\forall(d\in\R^n,\
	x\in B_\epsilon(\bar x))
$$

\Theorem{2.1.8}{Sufficient optimality condition}\label{b43d95d}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. Moreover, let $\bar x$ be a stationary point of $f$
such that $\nabla^2f(\bar x)$ is positive definite. Then $\bar x$ is a
strict local minimizer of $f$.

\Theorem{2.2.1}{First-order characterizations}\label{cd9cea7}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be
differentiable on $C$. Then the following hold for all $x,\bar x\in
	C$:
\begin{enumerata}
	\item $f$ is convex on $C$ iff
	\begin{equation*}
		f(x)\geq f(\bar x)+\nabla f(\bar x)^T(x-\bar x)\Tag{*}
	\end{equation*}
	\item $f$ is strictly convex on $C$ iff $(*)$ holds with strict
	inequality whenever $x\neq\bar x$.
	\item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff
	$$
		f(x)\geq\Big[f(\bar x)+\nabla f(\bar x)^T(x-\bar x)\Big]
		+\frac\mu2\norm{x-\bar x}^2
	$$
\end{enumerata}

\Corollary{2.2.2}{}\label{aa63a8a}

Let $f:\R^n\to\R$ be convex and differentiable. Then the following
hold:
\begin{enumerata}
	\item \textit{(Affine minorization)} There exists an
	\href{dcb7f73}{affine} function $g:\R^n\to\R$ which minorizes
	$f$ everywhere, i.e.
	$$
		g(x)\leq f(x)\with(x\in\R^n)
	$$
	\item If $f$ is strongly convex then it is strictly convex and
	coercive (level-bounded).
\end{enumerata}

\Corollary{2.2.3}{}\label{f2986e2}

Let $f:\R^n\to\R$ be convex and differentiable. Then the following
are equivalent:
\begin{enumerati}
	\item $\bar x$ is a global minimizer of $f$, i.e. $\bar x\in\argmin f$;
	\item $\bar x$ is a local minimizer;
	\item $\bar x$ is a stationary point of $f$.
\end{enumerati}

\begin{proof}
	(i) $\implies$ (ii) is obvious. (ii) $\implies$ (iii) follows from
	\href{dc165c9}{Fermat's Theorem}. (iii) $\implies$ (i) follows
	from \href{cd9cea7}{Theorem 2.2.1 (a)}.
\end{proof}

\Corollary{2.2.4}{Monotonocity of gradient mappings}\label{a8ddd0c}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be
differentiable on $C$. Then the following hold for all $x,y\in C$
\begin{enumerata}
	\item $f$ is convex on $C$ iff
	\begin{equation*}
		\inner{\nabla f(x)-\nabla f(y)}{x-y}\geq0\Tag{*}
	\end{equation*}
	\item $f$ is strictly convex on $C$ iff $(*)$ holds with a strict
	inequality whenever $x\neq y$.
	\item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff
	$$
		\inner{\nabla f(x)-\nabla f(y)}{x-y}\geq\mu\norm{x-y}^2
	$$
\end{enumerata}

\Theorem{2.2.5}{Twice differentiable convex functions}\label{eeb9c30}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be twice
continuously differentiable on $C$. Then the following hold:
\begin{enumerata}
	\def\allx{\forall x\in C}
	\def\hessian{\nabla^2f(x)}
	\item $f$ is convex on $C$ iff $\hessian$ is positive
	semidefinite $\allx$.
	\item If $\hessian$ is positive definite $\allx$ then $f$ is
	strictly convex on $C$.
	\item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff,
	$\allx$, the smallest eigenvalue of $\hessian$ is bounded by $\mu$
	from below.
\end{enumerata}

\Corollary{2.2.6}{Convexity of quadratic functions}\label{de25005}

Let $A\in\R^{n\times n}$ be symmetric and $b\in\R^n$, $\gamma\in\R$,
and define $f:\R^n\to\R$ by
$$
	f(x)=\frac12x^TAx+b^Tx+\gamma
$$
Then the following hold:
\begin{enumerata}
	\item $f$ is convex if and only if $A$ is positive semidefinite
	\item $f$ is strongly convex if and only if $A$ is positive definite
\end{enumerata}

\begin{proof}
	% TODO: come back here and show that f is twice cont diff.

	In view of \href{eeb9c30}{Theorem 2.2.5}, it suffices to show that $f$ is
	twice continuously differentiable.
	\begin{align*}
		\nabla f(x)  & = Ax + b \\
		\nabla^2f(x) & = A
	\end{align*}
	and we are done.
\end{proof}

\Theorem{2.2.7}{Convex optimization}\label{f546fc9}

Let $f:\R^n\to\R$ be a convex function and $X\subset\R^n$ be a
non-empty convex set. Consider the convex optimization problem
\begin{equation*}
	\begin{array}{l l l}
		\min f(x) & \text{s.t.} & x\in X\Tag{*}
	\end{array}
\end{equation*}
Then the following hold:
\begin{enumerata}
	\item A point $\bar x$ is a global minimizer of $(*)$ if and only if
	it is a local minimizer of $(*)$.
	\item The solution set $\argmin_Xf$ of $(*)$ is convex (possibly
	empty).
	\item If $f$ is strictly convex, then the solution set has at most
	one element.
	\item If $f$ is strongly convex and differentiable and $X$ is closed, then $(*)$ has exactly
	one solution ($\argmin_Xf$ is a singleton).
\end{enumerata}

\Proposition{2.3.1}{Operator norms}\label{bad47a4}

Let $\norm\cdot_*$ be a (vector) norm on $\R^n$ and $\R^m$,
respectively. Then for $A\in\R^{m\times n}$,
$$
	\norm A_*:=\sup_{x\neq0}\frac{\norm{Ax}_*}{\norm x_*}
$$
is a norm on $\R^{m\times n}$ with
$$
	\norm A_*=\sup_{\norm x_*=1}\norm{Ax}_*=\sup_{\norm x_*\leq1}\norm{Ax}_*
$$

\Proposition{2.3.2}{}\label{ab2107f}

Let $A\in\R^{m\times n}$. Then we have
$$
	\begin{array}{l l l}
		\norm{A}_1      & = \displaystyle\max_{j=\iter1n}\sum_{i=1}^m|a_{ij}| & \text{(maximum absolute column sum)} \\[1.5em]
		\norm{A}_2      & = \displaystyle\sqrt{\lambda_{\max}(A^TA)}          & \text{(spectral norm)}               \\[1em]
		\norm{A}_\infty & = \displaystyle\max_{i=\iter1m}\sum_{j=1}^n|a_{ij}| & \text{(maximum absolute row sum)}
	\end{array}
$$

\Proposition{2.3.3}{}\label{dd47a09}

Let $\norm\cdot_*$ be a norm on $\R^n$, $\R^m$, and $\R^p$
respectively. Then for all $A\in\R^{m\times n}$ and $B\in\R^{n\times
		p}$ the following hold:
\begin{itemize}
	\def\nm#1{\norm{#1}_*}
	\item $\nm{Ax}\leq\nm A\nm x$ for all $x\in\R^n$ \quad \textit{(compatibility)}
	\item $\nm{AB}\leq\nm A\nm B$ \quad \textit{(submultiplicativity)}
\end{itemize}

\Proposition{2.3.4}{Banach Lemma}\label{c733e75}

Let $C\in\R^{n\times n}$ with $\norm C<1$ where $\norm\cdot$ is a
submultiplicative matrix norm. THen $I+C$ is invertible and we have
$$
	\norm{(I+C)^{-1}}\leq\frac1{1-\norm C}
$$

\Corollary{2.3.5}{}\label{ae04fae}

Let $A,B\in\R^{n\times n}$ with $\norm{I-BA}<1$ for some
submultiplicative norm $\norm\cdot$. Then $A$ and $B$ are invertible
with
$$
	\norm{B^{-1}}\leq\frac{\norm A}{1-\norm{I-BA}}
$$

\Definition{3.1.1}{Descent direction}\label{ac99a6d}

Let $f:\R^n\to\R$ and $x\in\R^n$. A vector $d\in\R^n$ is said to be a
descent direction of $f$ at $x$ if there exists $\eta>0$ such that
$$
	f(x+td)<f(x)\with(t\in(0,\eta])
$$

\Proposition{3.1.2}{Sufficient condition for descent direction}\label{f81d53c}

Let $f:\R^n\to\R$ be directionally differentiable at $x\in\R^n$ in the
direction $d\in\R^n$ with
$$
	f'(x;d)<0
$$

Then $d$ is a descent direction of $f$ at $x$. In particular, this is
true if $f$ is differentiable at $x$ with
$$
	\nabla f(x)^Td<0
$$

\begin{proof}
	The first statement follows immediately from the definition of the
	\href{37eb747}{directional derivative}. The second one uses
	\href{ed67d29}{Lemma 2.1.2}.
\end{proof}

\Corollary{3.1.3}{}\label{c41d0f0}

Let $f:\R^n\to\R$ be differentiable, $B\in\R^{n\times n}$ be positive
definite and $x\in\R^n$ with $\nabla f(x)\neq0$. Then $-B\nabla f(x)$
is a descent direction of $f$ at $x$.

\begin{proof}
	This result follows almost immediately from the definition of a
	\href{37eb747}{descent direction} and the definition of a
	\href{e25e722}{positive definite} matrix.
\end{proof}

\Definition{3.1.4}{Step-size rule}\label{ae4eac6}

Let $f:{\R}^n\to{\R}$ be continuously differentiable and
let $\mathcal A_f:=\{(x,d)\mid\nabla f(x)^Td<0\}$. A set-valued
mapping
$$
	T:(x,d)\in\mathcal A_f\mapsto T(x,d)\subset\R_{++}
$$
%
% is called a step-size rule for $f$.
%
% We call it well-defined for $f$ if $T(x,d)\neq0$ for all
% $(x,d)\in\mathcal{A}_f$.
%
% If the step-size rule is well-defined for all continuously
% differentiable functions ${\R}^n\to{\R}$, we simply call it
% well-defined.
%
% \Definition{3.1.5}{Efficient step-size}\label{d23fdf0}
%
% Let $f:\R^n\to\R$ be continuously differentiable. The
% \href{ae4eac6}{step-size rule} $T$ is called efficient for $f$ if
% there exists $\theta>0$ such that
% $$
% 	f(x+td)\leq f(x)-\theta\left(\frac{\nabla f(x)^Td}{\norm d}\right)^2
% $$
%
% \Theorem{3.1.6}{Global convergence of general descent method}
%
% Let $f:\R^n\to\R$ be continuously differentiable, and let $\{x^k\}$,
% $\{d^k\}$, $\{t_k\}$ be generated by \href{edbf62c}{Algorithm 3.1.1}.
% Moreover, assume that the following hold:
% \begin{enumerati}
% 	\item \textit{(Angle condition)} There exists $c>0$ such that
% 	$$
% 		-\frac{\nabla f(x^k)^Td^k}{\norm{\nabla f(x^k)}\cdot\norm{d^k}}\geq c\with\forall k\in\N
% 	$$
%
% 	i.e. the angle between the gradient vector and the descent direction
% 	is at most $90^\circ$.
% 	\item \textit{(\href{d23fdf0}{Efficient step-size})} There exists
% 	$\theta>0$ such that
% 	$$
% 		f(x^k+t_kd^k)\leq f(x^k)-\theta\left(
% 		\frac{\nabla f(x^k)^Td^k}{\norm{d^k}}
% 		\right)^2\with\forall k\in\N
% 	$$
% \end{enumerati}
% Then every cluster point of $\{x^k\}$ is a stationary point of $f$.
%
% \begin{proof}
% 	By (ii), there exists $\theta>0$ such that
% 	$$
% 		f(x^{k+1})=f(x^k+t_kd^k)\leq
% 		f(x^k)-\theta\left(
% 		\frac{\nabla f(x^k)^Td^k}{\norm{d^k}}
% 		\right)^2\with\forall k\in\N
% 	$$
% 	Putting $k:=c^2\theta$, the angle condition implies
% 	\begin{equation*}
%     f(x^{k+1})\leq f(x^k)-k\norm{\nabla f(x^k)}^2\Tag{*}
% 	\end{equation*}
%
%   Let $\bar x$ be a cluster point of $\{x^k\}$. As $\{f(x^k)\}$ is
%   monotonically decreasing (from ($*$)) and convergent to $f(\bar x)$
%   on a subsequence (since $\{x^k\}\to\bar x$ on a subsequence and $f$
%   is continuous), this implies that the whole sequence $\{f(x^k)\}$
%   converges to $f(\bar x)$. 
% \end{proof}

\Definition{3.2.0}{Armijo rule and sufficient decrease}\label{fefb024}

Choose $\beta,\sigma\in(0,1)$. For $x,d\in\href{ae4eac6}{\mathcal
		A_f}$ the Armijo rule $T_A$ is defined by
$$
	T_A(x,d)=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq
	f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
$$

The inequality
$$
	f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k
$$

is called the \textit{Armijo condition}. It ensures a
\textbf{sufficient decrease} on the objective function.

\Example{3.2.1}{Insufficient decrease}\label{ae7f42d}

Consider the function $f(x)=(x-1)^2-1$ with optimal value $f^*=-1$.

The sequence $\{x_k\}$ with $x_k:=-\frac1k$ has
$f(x_k)=\frac{1+2k}{k^2}$ and
$$
	f(x_{k+1})-f(x_k)=\frac{2k^2+4k+1}{k^2(k+1^2)}<0
$$

Hence we've found a case where the objective value decreases, but
$f(x_k)$ converges to a non-optimal value. ($f(x_k)\to0$, but we want
$f(x_k)\to-1$)

\Lemma{3.2.3}{Convergence to gradient}\label{f8e1f12}

Let $f:\R^n\to\R$ be continuously differentiable. Moreover, let
$\{x^k\in\R^n\}\to x$, $\{d^k\in\R^n\}\to d$ and
$\{t_k>0\}\downarrow0$. Then
$$
	\lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=\nabla f(x)^Td
$$

\begin{proof}
	By the \href{d37aa2b}{mean value theorem}, for all $k\in\N$, there
	exists $\eta^k\in[x^k,x^k+t_kd^k]$ such that
	$$
		f(x^k+t_kd^k)-f(x^k)=t_k\nabla f(\eta^k)^Td^k
	$$
	Clearly, $\eta^k\to x$ and hence the continuity of $\nabla f$ yields
	$$
		\nabla f(\eta^k)^Td^k\to\nabla f(x)^Td
	$$
	This readily implies
	$$
		\lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=
		\lim_{k\to\infty}\nabla f(\eta^k)^Td^k=
		\nabla f(x)^Td
	$$
\end{proof}

\Theorem{3.2.4}{Global convergence of the gradient method}\label{bbb25cd}

Let $f:\R^n\to\R$ be continuously differentiable. Then every cluster
point of a sequence generated by the \href{ae01f6d}{Gradient method
	with Armijo rule} is a stationary point of $f$.

\begin{proof}
	\def\xk{\{x^k\}}
	\def\grad#1{\nabla f(#1)}
	Assume on the contrary that $\grad{\bar x}\neq0$.

	Let $\bar x$ be a cluster point of the generated sequence $\xk$, and
	let $\xk_K$ be a subsequence converging to $\bar x$. By the
	continuity of $f$, $\{f(x^k)\}\to_Kf(\bar x)$.

	As $\{f(x^k)\}$ is monotonically decreasing by the Armijo condition
	and converges on a subsequence to $f(\bar x)$, \href{aaf3ba6}{by
		inspection}, $\{f(x^k)\}_\N$ converges to $f(\bar x)$.

	In particular, we have
	$$f(x^k)-f(x^{k+1})\to0$$

	Substituting $t_k=\beta^l$ and $x^{k+1}=x^k+\beta^ld^k$ into steps
	\textbf{(S2)} and \textbf{(S3)} of the algorithm, we have
	$$
		0\leq
		t_k\norm{\grad{x^k}}^2=
		-t_k\grad{x^k}^Td^k\leq
		\frac{f(x^k)-f(x^{k+1})}\sigma\to0
	$$

	Since $\grad{x^k}\to_K\grad{\bar x}\neq0$ (by continuity of $\nabla
		f$), by squeeze theorem on the above inequality, this implies that
	$t_k\to_K0$. Due to \textbf{(S3)}, for all $k\in K$ sufficiently
	large, we have
	\begin{equation*}
		f(x^k+\beta^{l_k-1}d^k)-f(x^k)>\beta^{l_k-1}\sigma\grad{x^k}^Td^k\tag*{($*$)}
	\end{equation*}

	where $\beta^{l_k}=t_k$ and $l_k\in\N$ is the exponent
	\textit{uniquely} determined by the Armijo rule in \textbf{(S3)}.
	Note that $l_k$ is the smallest value of $l$ that satisfies the
	\href{fefb024}{Armijo condition}, and hence $l_k-1$ does \textit{not}
	satisfy the Armijo condition, hence $(*)$.

	Passing to the limit on $K$ and using \href{f8e1f12}{Lemma 3.2.3}
	gives
	$$
		-\norm{\nabla f(\bar x)}^2\geq-\sigma\norm{\nabla f(\bar x)}^2
	$$
	Which is a contradiction because $\sigma\in(0,1)$ and $\nabla f(\bar
		x)\neq0$ by assumption. Hence, $\bar x$ is indeed a stationary point
	of $f$, completing the proof.
\end{proof}

\Theorem{3.3.9}{Global convergence of \href{a7a5665}{Algorithm 3.3.3}}\label{a66db73}

Let $f:\R^n\to\R$ be twice continuously differentiable. Then every
cluster point of a sequence generated by \href{a7a5665}{Algorithm
	3.3.3} is a stationary point of $f$.

\begin{proof}

	\def\xk{\{x^k\}}
	\def\grad{\nabla f(x^k)}

	Let $\xk$ be generated by \href{a7a5665}{Algorithm 3.3.3}. Let
	$\bar x$ be a cluster point with $\xk\to_K\bar x$. (This exists
	because $\xk$ is monotone decreasing and hence there exists a
	subsequence $\xk_K$ that converges to any given cluster point)

	If $d^k=\grad$ for infinitely $k\in K$ then the assertion follows immediately from
	% TODO: revisit this after completing Remark 3.2.5

\end{proof}

\Remark{x.x.x}{Collection of unconstrained minimization methods}\label{a75d03d}

\begin{enumerate}
	\item Gradient method
	\item Globalized Newton's method
	\item Globalized BFGS method
	\item Globalized inexact Newton's method
\end{enumerate}

\Algorithm{3.1.1}{General line-search descent algorithm}\label{edbf62c}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] \textit{Initialization}: Choose $x^0\in\R^n$ and put $k:=0$.
	\item [\textbf{(S1)}] \textit{Termination}: If $x^k$ satisfies a termination criterion: STOP.
	\item [\textbf{(S2)}] \textit{Search direction}: Determine $d^k$ such that $\nabla f(x^k)^Td^k<0$.
	\item [\textbf{(S3)}] \textit{Step-size}: Determine $t_k>0$ such that $f(x^k+t_kd^k)<f(x^k)$.
	\item [\textbf{(S4)}] \textit{Update}: Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.2.1}{Gradient method with Armijo rule}\label{ae01f6d}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\sigma,\beta\in(0,1)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Put $d^k:=-\nabla f(x^k)$.
	\item [\textbf{(S3)}] Determine $t_k>0$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.1}{Local Newton's method for equations}\label{abbc9be}

Goal is to solve
$$F(x)=0$$

where $F:\R^n\to\R^n$ and $F$ is assumed to be continuously differentiable.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{F(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$F'(x^k)d=-F(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.2}{Local Newton's method for unconstrained optimization}\label{c2bcc4e}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\href{abbc9be}{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.3}{Globalized Newton's method for unconstrained optimization}\label{a7a5665}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\href{abbc9be}{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\rho>0$, $p>2$, $\beta\in(0,1)$, $\sigma\in(0,\frac12)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Try to compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	      If no solution can be found or if
	      \begin{gather*}
		      \nabla f(x^k)^Td^k>-\rho\norm{d^k}^p\\
		      \text{\small{(\href{fefb024}{insufficient decrease})}}
	      \end{gather*}
	      then fall back to $d^k:=-\nabla f(x^k)$
	\item [\textbf{(S3)}] Determine $t_k$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Definition{5.0.0}{Standard Nonlinear Program}\label{bbe9993}

\begin{equation*}
	\begin{array}{l l l l}
		\displaystyle \min_{x\in\R^n}f(x)
		 & \text{s.t.} & g_i(x)\leq0 & \forall i\in I \\
		 &             & h_j(x)=0    & \forall j\in J
	\end{array}\Tag{*}
\end{equation*}
Where $f,g_i,h_j:\R^n\to\R$ are continuously differentiable.

We call $(*)$ a nonlinear program (NLP) in standard form.

By convention, we let the feasible set of $(*)$ be denoted by $X$, with
\begin{equation*}
	X:=\Set{x\in\R^n}{\begin{array}{l l}
			g_i(x)\leq0 & \forall i\in I \\
			h_j(x)=0    & \forall j\in J
		\end{array}\Tag{**}}
\end{equation*}

By the continuity of the constraint functions, $X$ is closed.

We will use $I:=\{\iter1m\}$ and $J:=\{\iter1p\}$, and define the
\textbf{active set} at $\bar x\in X$ as
$$
	I(\bar x):=\Set{i\in I}{g_i(\bar x)=0}
$$

\Theorem{5.1.5}{}\label{c8e5836}

Let $\bar x$ be a local minimum of $f\in C^1$ over $S$. Then
$$
	\nabla f(\bar x)^Td\geq0\quad\forall(d\in T_S(\bar x))
$$
% In whichever direction you choose to walk from x bar, you will
% increase the objective value

\Theorem{5.1.9}{Farkas Lemma}\label{d64b0db}

Let $B\in\R^{m\times n}$, and $h\in\R^n$. The following are equivalent:
\begin{enumerate}
	\item The system $B^Tx=h$ where $x\geq0$ (element-wise) has a solution
	\item $\forall d: Bd\geq0\implies h^Td\geq0$
\end{enumerate}

\begin{proof}
	Proving that (1) implies (2).

	Let $x\geq0$ such that $B^Tx=h$.

	Then for any $d$ such that $Bd\geq0$, we have
	$$
		h^Td = (B^Tx)^Td = x^TBd
	$$

	But $x^TBd\geq0$ because $x\geq0$ and $Bd\geq0$.

	Proving that (2) implies (1) by contrapositive.

	Assume that (1) is false. Then
	$$
		h\notin\{B^Tx \mid x\geq0\}=:K
	$$

	% TODO: add this lemma
	$K$ is a closed convex cone. (closedness follows from Lemma 5.1.8)

	Set $\bar s:=P_K(h)$ and $\bar d:=\bar s-h\neq0$. (Note that $\bar
		s\in K$)

	By Proposition 5.1.7(c),
	$$
		\bar d^T(s-\bar s)\geq0\quad\forall(s\in K)
	$$

	So then with $s:=0$, we get $\bar d^T\bar s\leq0$, and with
	$s:=2\bar s$, we get $\bar d^T\bar s\geq0$ and hence
	$$
		\bar d^T\bar s=0
	$$

	And hence
	$$
		\bar d^Ts\geq0
	$$

	Then by definition of cone $K$,
	\begin{align*}
		\bar d^T B^Tx         & \geq0\quad\forall(x\geq0) \\
		\implies (B\bar d)^Tx & \geq 0
	\end{align*}


	Inserting $x:=e_i$ (where $e_i$ is the $i^\text{th}$ component
	vector) for $i=\iter1n$ implies $(B\bar d)^T\geq0$

	On the other hand
	\begin{align*}
		h^T\bar d & = (\bar s-\bar d)^T\bar d        \\
		          & = \bar s^T\bar d-\norm{\bar d}^2 \\
		          & = -\norm{\bar d}^2 \leq 0
	\end{align*}

	But since $\bar d\neq0$, we have the strict inequality $h^T\bar d<0$.

	Therefore, $B\bar d\geq0$, but $h^T\bar d<0$, i.e. (2) does not
	hold.
\end{proof}

\Definition{5.1.10}{Karush-Kuhn-Tucker conditions}\label{b38093d}

Consider the (standard) NLP:

\begin{equation*}
	\min_{x\in\R^n} f(x)\quad\text{s.t.}\quad\begin{array}{l}
		g_i(x)\leq0 \quad\forall i=\iter1m \\
		h_j(x)=0    \quad\forall j=\iter1p
	\end{array}
	\tag*{(1)}
\end{equation*}

\begin{enumerate}
	\item The function $L:\R^n\times\R^m\times\R^p\to\R$ defined by
	      $$
		      L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_ig_i(x)+\sum_{j=1}^p\mu_jh_j(x)
	      $$
	      is called the Langrangian (function) of \textbf{(1)}.
	\item The set of conditions
	      \begin{align*}
		      \nabla_xL(x,\lambda,\mu)                 & = 0 \\
		      h(x)                                     & = 0 \\
		      \lambda\geq0,\ g(x)\leq0,\ \lambda^Tg(x) & =0
	      \end{align*}
	      are called the Karush-Kuhn-Tucker conditions for \textbf{(1)},
	      where
	      $$\nabla_xL(x,\lambda,\mu)=\nabla f(x)+\sum_{i=1}^m\lambda_i\nabla g_i(x)+\sum_{j=1}^p\mu_j\nabla h_j(x)$$
	\item A triple $(\bar x,\bar\lambda,\bar\mu)$ that satisfies the
	      KKT conditions is called a KKT point.
	\item Give $\bar x$, feasible for \textbf{(1)}, we define
	      $$
		      M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of \textbf{(1)}} \}
	      $$
	      the set of all KKT multipliers at $\bar x$.
\end{enumerate}

\Definition{5.1.11}{Linearized cone}\label{ca4f471}

Let $X$ be the feasible set of \textbf{(1)}. The linearized cone (of
$X$) at $\bar x\in X$ is defined by
$$
	L_X(\bar x):=\left\{ d\in\R^n\ \middle\vert
	\begin{array}{l l}
		\nabla g_i(x)^Td\leq0 & \forall i=\iter1m \\
		\nabla h_j(x)=0       & \forall j=\iter1p
	\end{array}
	\right\}
$$

\Definition{5.1.12}{Abadie constraint qualification}\label{adc266e}

We say that the Abadie constraint qualification (ACQ) holds at $\bar
	x\in X$ if
$$
	T_X(\bar x)=L_X(\bar x)
$$

\Theorem{5.1.13}{\href{b38093d}{KKT} under \href{adc266e}{ACQ}}\label{b1c5437}

Let $\bar x\in X$ (where $X$ is the feasible set) be a local minimum
of \textbf{(1)} such that \href{adc266e}{ACQ} holds at $\bar x$. Then
there exists $(\bar\lambda,\bar\mu)\in\R^m\times\R^p$ such that $(\bar
	x,\bar\lambda,\bar\mu)$ is a \href{b38093d}{KKT} point of
\textbf{(1)}

\begin{proof}
	\def\bm{\bar\mu}\def\bl{\bar\lambda}\def\bx{\bar x}
	By Theorem 5.1.5,
	\begin{equation*}
		\nabla f(\bx)^Td\geq0\quad\forall(d\in T_X(\bx))\tag*{($*$)}
	\end{equation*}

	Set
	$$
		B:=\begin{pmatrix}
			-\nabla g_i(\bx)^T\quad(i=\iter1m) \\
			-\nabla h_j(\bx)^T\quad(j=\iter1p) \\
			\nabla h_j(\bx)^T\quad(j=\iter1p)
		\end{pmatrix}\in\R^{(m+2p)\times{n}}
	$$
	%TODO: check the sign of the 2nd and 3rd row

	Then $d\in L_X(\bx)\iff Bd\geq0$.

	By ACQ, we have $d\in T_X(\bx)\iff Bd\geq0$

	Combined with $(*)$, we have
	$$
		\nabla f(\bx)^Td\geq0\quad\forall(d:Bd\geq0)
	$$

	(Think $h=\nabla f(\bx)$ and apply the Farkas Lemma.)

	By the Farkas Lemma,
	$$
		\exists y=\begin{pmatrix}y^1\in\R^m\\y^2\in\R^p\\y^3\in\R^p\end{pmatrix}
	$$

	such that $y\geq0$, and $B^Ty=\nabla f(\bx)$

	Define $\bl\in\R^n,\bm\in\R^p$ by
	$$
		\bl_i=\begin{cases}
			y^1_i & \text{if $i=\iter1m$} \\
			0     & \text{otherwise}
		\end{cases}
	$$
	and
	$$
		\bm_i=\begin{cases}
			y^2_j - y^3_j & \text{if $j=\iter{m+1}{m+2p}$} \\
			0             & \text{otherwise}
		\end{cases}
	$$

	Then $(\bx,\bl,\bm)$ is a KKT point.

	MORE NOTES

	\begin{align*}
		0
		 & = \nabla f(\bx)
		+\sum_{i=0}^my^1_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}(y^2_j-y^3_j)\nabla h_j(\bx) \\
		 & = \nabla f(\bx)
		+\sum_{i=0}^m \bl_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}\bm_j\nabla h_j(\bx)
	\end{align*}

	and then there is a line with a tick/check next to it:
	$$
		\bl^Tg(\bx)=\sum_{i=0}^m\bl_ig_i(\bx)=0
	$$
\end{proof}

\Definition{}{Standard NLP}\label{befc1f5}
\begin{equation*}
	\min_{x\in\R^n} f(x)\quad\text{s.t.}\quad\begin{array}{l}
		g_i(x)\leq0 \quad\forall i=\iter1m \\
		h_j(x)=0    \quad\forall j=\iter1p
	\end{array}
	\tag*{(1)}
\end{equation*}

Let $I:=\iter1m$ and $J:=\iter1p$.

We define the active set $I(\bar x)$ as
$$
	I(\bar x):=\{i\in I\mid g_i(x) = 0\}
$$

Recall:
$$
	T_X(\bar x):=\left\{
	d\in\R^n \;\middle\vert\; \exists \{x^k\in X\}\to\bar x,\{t_k\}\downarrow0:\frac{x^k-\bar x}{t_k}\to d
	\right\}
$$
This is the space where you can take a miniscule step from $\bar x$
and still remain in the feasible set. (The tangent cone)

Also recall:
$$
	L_X(\bar x):=\left\{
	d\in\R^n
	\;\middle\vert\;
	\begin{array}{l l}
		g_i(\bar x)\leq0 & \forall i\in I(\bar x) \\
		h_j(\bar x)=0    & \forall j\in J
	\end{array}
	\right\}
$$
This is the linearized cone.

\Definition{5.1.14}{Constraint qualifications}\label{e8fa554}

A condition of $X$ (i.e. on $g$ and $h$) that ensures that the KKT
conditions hold at a local minimizer is called a constraint
qualification.

\Definition{5.1.15}{}\label{fed784a}

Let $\bar x$ be feasible for (1). We say that

\begin{enumerata}
	\item the linear independence constraint qualification (LICQ) holds
	at $\bar x$ if
	$$
		\nabla g_i(\bar x)\quad\forall(i\in I(\bar x)),\ \nabla h_j(\bar x)\quad\forall(j\in J)
	$$
	are linearly indepdendent
	\item the Mangasarian-Fromovitz constraint qualification (MFCQ)
	holds at $\bar x$ if
	$$
		\nabla h_j(\bar x)\quad\forall(j\in J)
	$$
	are linearly indepdendent, and
	\begin{gather*}
		\nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
		\nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
	\end{gather*}
\end{enumerata}

\Proposition{5.1.16}{LICQ $\implies$ MFCQ}\label{a7ef3f5}

Let $\bar x$ be feasible for (1) such that LICQ holds at $\bar x$.
Then $MFCQ$ holds.

\Lemma{5.1.17}{}\label{a9bea04}

Let $\bar x$ be feasible for (1) such that there exists $d\in\R^n$
such that
$$
	\nabla h_j(\bar x)\quad\forall(j\in J)
$$
are linearly indepdendent, and
\begin{gather*}
	\nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
	\nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
\end{gather*}

Then there exists $\epsilon>0$ and a $C^1$-curve
$\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
\begin{gather*}
	\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
	\gamma(0) = \bar x \\
	\gamma'(0)=d
\end{gather*}


\Lemma{5.1.18}{}\label{a08cd35}

Let $\bar x$ be feasible for (1). Then
$$
	T_X(\bar x)\subset L_X(\bar x)
$$

\begin{proof}
	Exercise 9.3
\end{proof}

\Proposition{5.1.19}{MFCQ $\implies$ ACQ}\label{eddce03}

Let $\bar x$ be feasible for (1) such that MFCQ holds at $\bar x$.
Then ACQ holds at $\bar x$.

\begin{proof}
	Only need to show $L_X(\bar x)\subset T_X(\bar x)$.

	Let $d\in L_X(\bar x)$. By MFCQ, there exists $\hat d$ such that
	% Note: we can't use d directly instead of d(∂) because d doesn't
	% have the strict inequality required by Lemma 5.1.17
	\begin{gather*}
		\nabla g_i(\bar x)^T\hat d<0\quad\forall(i\in I(\bar x)) \\
		\nabla h_j(\bar x)^T\hat d=0\quad\forall(j\in J)
	\end{gather*}
	and $\nabla h_j(\bar x)\ \forall(j\in J)$ are linearly indepdendent.

	Set $d(\delta):= d + \delta\hat d$. Then, $\forall\delta>0$:
	\begin{gather*}
		\nabla g_i(\bar x)^T{d(\delta)}<0\quad\forall(i\in I(\bar x)) \\
		\nabla h_j(\bar x)^T{d(\delta)}=0\quad\forall(j\in J)
	\end{gather*}

	Applying Lemma 5.1.17 to $d(\delta)$ yields a $C^1$-curve
	$\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
	\begin{gather*}
		\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
		\gamma(0) = \bar x \\
		\gamma'(0)=d(\delta)
	\end{gather*}

	Let $\{t_k\}\downarrow0$ and set $x^k:=\gamma(t_k)$. Then
	$\{x^k\in X\}\to\bar x$.

	And $d(\delta)=\gamma'(0)$ and hence
	$$
		d(\delta)=\lim_{k\to\infty}\frac{\gamma(t_k)-\gamma(0)}{t_k}=
		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}
	$$

	And since
	$$
		\lim_{\delta\to0} d(\delta) = d
	$$
	and both lie in $T_X(\bar x)$ due to an unproved argument using
	closedness.

	this completes the proof with
	$$
		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}=d
	$$
\end{proof}

\Corollary{3.1.20}{KKT under MFCQ}\label{b39f0fe}

Let $\bar x$ be a local minimum of (1) such that MFCQ holds at $\bar x$. Then:

\begin{enumerate}\renewcommand{\theenumi}{\alph{enumi}}
	\item There exists $(\bar\lambda, \bar\mu)\in\R^m\times\R^p$  such
	      that $(\bar x,\bar\lambda, \bar\mu)$ is a KKT point of (1).
	\item $M(\bar x)$ is bounded where
	      $$
		      M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of \textbf{(1)}} \}
	      $$
\end{enumerate}

\begin{proof}
	Proving (a) requires Proposition 5.1.19 (MFCQ $\implies$ ACQ) and
	Theorem 5.1.13

	Proving (b):

	Suppose $M(\bar x)$ were unbounded, i.e. there exists
	$$
		\{(\lambda^k,\mu^k)\in M(\bar x)\} : \norm{(\lambda^k,\mu^k)}\to+\infty
	$$

	Then WLOG,
	$$
		\frac{(\lambda^k,\mu^k)}{\norm{(\lambda^k,\mu^k)}}\to(\tilde\lambda,\tilde\mu)
	$$

	Note that every element in the above sequence has norm 1. And hence
	we know that it doesn't converge to the zero vector, since that has
	norm 0.

	Since $(\bar x,\lambda^k,\mu^k)$ is a KKT point of (1), we have
	$$
		0 = \frac{\nabla f(\bar x) + \sum_{i\in I(\bar x)}\lambda^k_i\nabla g_i(\bar x)
			+ \sum_{j\in J}\mu^k_j\nabla h_j(\bar x)}{\norm{(\lambda^k,\mu^k)}}
	$$

	Then as $h\to\infty$,

	\begin{equation*}
		0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)
		+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)
	\end{equation*} % cognitive, but just raw algebra

	% Note: taking ||.|| of a product space (X × Y)
	% ||(a,b)|| = ||a||+||b||
\end{proof}

Now multiply with $d$ from MFCQ at $\bar x$.
\begin{equation*}
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
	+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td \tag*{($*$)}
\end{equation*}

And by MFCQ, the second term is $=0$. Hence
$$
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
$$

\textbf{Case 1} $\exists i_0 \in I(\bar x): \tilde\lambda_{i_0}>0$. Then
$0<0$. Contradiction!
$$
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
	\;\leq\;\tilde\lambda_{i_0}\nabla g_{i_0}(\bar x)^Td<0
$$
\textbf{Case 2} $\forall i\in I(\bar x):\tilde\lambda_i=0$. Then ($*$) reads
$$
	0=\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td
$$
which is a contradiction against MFCQ.

\Corollary{5.1.21}{}\label{c55a5f3}

Let $\bar x$ be a local minimum of (1) such that LICQ holds at $\bar
	x$. Then:
\begin{enumerata}
	\item There exists $(\bar\lambda,\bar\mu)\in M(\bar x)$.
	\item $M(\bar x) = \{(\bar\lambda,\bar\mu)\}$.
\end{enumerata}

\begin{proof}
	(a) follows from Proposition 5.1.16 + Corollary 5.1.19

	(b)

	Assume that $(\tilde\lambda,\tilde\mu)\in M(\bar x)$. Then
	\begin{align*}
		0          & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)                             \\
		           & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\bar\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\bar\mu_j\nabla h_j(\bar x)                                 \\
		\implies 0 & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}(\tilde\lambda_i-\bar\lambda_i)\nabla g_i(\bar x)+\sum_{j\in J}(\tilde\mu_j-\bar\mu_j)\nabla h_j(\bar x)
	\end{align*}

	Then by LICQ,
	\begin{gather*}
		\tilde\lambda_i - \bar\lambda_i = 0\quad\forall(i\in I(\bar x)) \\
		\tilde\mu_j - \bar\mu_j = 0\quad\forall(j\in J) \\
	\end{gather*}

	Then since $\tilde\lambda_i = \bar\lambda_i\ \forall(i\notin I(\bar
		x))$, this shows $\tilde\lambda=\bar\lambda$ and $\tilde\mu=\bar\mu$.
\end{proof}
