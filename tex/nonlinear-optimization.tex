% vim:ft=tex

\section{Nonlinear Optimization}\label{d1c9db1}

\Definition{1.2.1}{Convex sets}

A set $C\subset\R^n$ is called convex if

$$
	\lambda x+(1-\lambda)y\in C\quad
	\forall(x,y\in C,\ \lambda\in(0,1))
$$

or simply a set which contains all connecting lines of points from the
set.

\Definition{1.2.3}{Convex functions}

Let $C\subset\R^n$ be convex. Let $\lambda\in(0,1)$ and $x,y\in C$ and
let

Then $f:C\to\R$ is said to be

\begin{itemize}
	\item convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$$
	\item strictly convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$$
	\item strongly convex on $C$ if $\exists\mu>0$ such that
	      $$f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)$$
\end{itemize}

\Proposition{1.2.6}{Convexity preserving operations}

\begin{enumerate}
	\item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
	      convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
	      convex.
	\item (Composition with affine mapping) $f:\R^n\to\R$ be convex
	      and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\Definition{2.1.1}{Directional derivative}\label{37eb747}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable
at $\bar x\in\R^n$ in the direction $d\in\R^n$ if
$$
	\lim_{t\downarrow0}\frac{f(\bar x+td)-f(x)}t
$$

exists. This limit is denoted by $f'(x;d)$ and is called the
directional derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we
call it directionally differentiable.

\Lemma{2.1.2}{Directional derivative and gradient}

Let $D\subset \R^n$ be open and $f:D\to\R$
differentiable at $x\in D$. Then $f$ is directionally differentiable
at $x$ with
$$
	f'(x;d)=\nabla f(x)^Td\quad\forall(d\in\R^n)
$$

Where $f'(x;d)$ is the \hyperref[37eb747]{directional derivative} of
$f$ at $x$ in the direction $d$.

\Lemma{2.1.4}{Basic optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local
minimizer of $f$ over $X$ and $f$ is \hyperref[37eb747]{directionally
	differentiable} at $\bar x$ then
$$
	f'(x;d)\geq 0\quad\forall(d\in\R^n)
$$

\Theorem{2.1.5}{Fermat's rule}

Let $X\subset\R^n$ be open and $f:X\to\R$ differentiable at $\bar x\in
	X$. If $\bar x$ is a local minimizer (or maximizer) of $f$ over $X$
then $\nabla f(\bar x)=0$.

\Theorem{2.1.6}{Second-order necessary condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x$ is a local minimizer (maximizer) of $f$
over $X$ then $\nabla^2f(\bar x)$ is positive (negative) semidefinite.

\Lemma{2.1.7}{}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x\in\R^n$ is such that $\nabla^2f(\bar x)$
positive definite then $\exists\,\mu,\epsilon>0$ such that
$B_\epsilon(\bar x)\subset X$ and
$$
	d^T\nabla^2f(x)d\geq\mu\norm d^2_2\quad\forall(d\in\R^n,\
	x\in B_\epsilon(\bar x))
$$

\Theorem{2.1.8}{Sufficient optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. Moreover, let $\bar x$ be a stationary point of $f$
such that $\nabla^2f(\bar x)$ is positive definite. Then $\bar x$ is a
strict local minimizer of $f$.

\Definition{3.1.4}{Step-size rule}\label{ae4eac6}

Let $f:{\R}^n\to{\R}$ be continuously differentiable and
let $\mathcal A_f:=\{(x,d)\mid\nabla f(x)^Td<0\}$. A set-valued
mapping
$$
	T:(x,d)\in\mathcal A_f\mapsto T(x,d)\subset\R_{++}
$$

is called a step-size rule for $f$.

We call it well-defined for $f$ if $T(x,d)\neq0$ for all
$(x,d)\in\mathcal{A}_f$.

If the step-size rule is well-defined for all continuously
differentiable functions ${\R}^n\to{\R}$, we simply call it
well-defined.

\Definition{3.1.5}{Efficient step-size}

Let $f:\R^n\to\R$ be continuously differentiable. The
\hyperref[ae4eac6]{step-size rule} $T$ is called efficient for $f$ if
there exists $\theta>0$ such that
$$
	f(x+td)\leq f(x)-\theta\left(\frac{\nabla f(x)^Td}{\norm d}\right)^2
$$

\Definition{3.2.0}{Armijo rule and sufficient decrease}\label{fefb024}

Choose $\beta,\sigma\in(0,1)$. For $x,d\in\hyperref[ae4eac6]{\mathcal
		A_f}$ the Armijo rule $T_A$ is defined by
$$
	T_A(x,d)=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq
	f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
$$

The inequality
$$
	f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k
$$

is called the \textit{Armijo condition}. It ensures a
\textbf{sufficient decrease} on the objective function.

\Example{3.2.1}{Insufficient decrease}

Consider the function $f(x)=(x-1)^2-1$ with optimal value $f^*=-1$.

The sequence $\{x_k\}$ with $x_k:=-\frac1k$ has
$f(x_k)=\frac{1+2k}{k^2}$ and
$$
	f(x_{k+1})-f(x_k)=\frac{2k^2+4k+1}{k^2(k+1^2)}<0
$$

Hence we've found a case where the objective value decreases, but
$f(x_k)$ converges to a non-optimal value. ($f(x_k)\to0$, but we want
$f(x_k)\to-1$)

\Theorem{3.2.4}{Global convergence of the gradient method}\label{bbb25cd}

Let $f:\R^n\to\R$ be continuously differentiable. Then every cluster
point of a sequence generated by the \hyperref[ae01f6d]{Gradient
method with Armijo rule} is a stationary point of $f$.

\begin{proof}
  \def\xk{\{x^k\}}
  \def\grad#1{\nabla f(#1)}

  Let $\bar x$ be a cluster point of the sequence $\xk$ generated, and
  let $\xk_K$ be a subsequence converging to $\bar x$. Now assume that
  $\grad{\bar x}\neq0$.

  As $\{f(x^k)\}$ is monotonically decreasing and $\{f(x^k)\}_K$
  converges to $f(\bar x)$, we infer that the whole sequence
  $\{f(x^k)\}$ converges to $f(\bar x)$

\end{proof}



\Theorem{3.3.9}{Global convergence of \hyperref[a7a5665]{Algorithm 3.3.3}}

Let $f:\R^n\to\R$ be twice continuously differentiable. Then every
cluster point of a sequence generated by \hyperref[a7a5665]{Algorithm
3.3.3} is a stationary point of $f$.

\begin{proof}

  \def\xk{\{x^k\}}
  \def\grad{\nabla f(x^k)}

  Let $\xk$ be generated by \hyperref[a7a5665]{Algorithm 3.3.3}. Let
  $\bar x$ be a cluster point with $\xk\to_K\bar x$. (This exists
  because $\xk$ is monotone decreasing and hence there exists a
  subsequence $\xk_K$ that converges to any given cluster point)

  If $d^k=\grad$ for infinitely $k\in K$ then the assertion follows immediately from 
  % TODO: revisit this after completing Remark 3.2.5

\end{proof}

\Remark{x.x.x}{Collection of unconstrained minimization methods}

\begin{enumerate}
	\item Gradient method
	\item Globalized Newton's method
	\item Globalized BFGS method
	\item Globalized inexact Newton's method
\end{enumerate}

\Algorithm{3.1.1}{General line-search descent algorithm}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] \textit{Initialization}: Choose $x^0\in\R^n$ and put $k:=0$.
	\item [\textbf{(S1)}] \textit{Termination}: If $x^k$ satisfies a termination criterion: STOP.
	\item [\textbf{(S2)}] \textit{Search direction}: Determine $d^k$ such that $\nabla f(x^k)^Td^k<0$.
	\item [\textbf{(S3)}] \textit{Step-size}: Determine $t_k>0$ such that $f(x^k+t_kd^k)<f(x^k)$.
	\item [\textbf{(S4)}] \textit{Update}: Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.2.1}{Gradient method with Armijo rule}\label{ae01f6d}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\sigma,\beta\in(0,1)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Put $d^k:=-\nabla f(x^k)$.
	\item [\textbf{(S3)}] Determine $t_k>0$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.1}{Local Newton's method for equations}\label{abbc9be}

Goal is to solve
$$F(x)=0$$

where $F:\R^n\to\R^n$ and $F$ is assumed to be continuously differentiable.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{F(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$F'(x^k)d=-F(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.2}{Local Newton's method for unconstrained optimization}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\hyperref[abbc9be]{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.3}{Globalized Newton's method for unconstrained optimization}\label{a7a5665}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\hyperref[abbc9be]{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\rho>0$, $p>2$, $\beta\in(0,1)$, $\sigma\in(0,\frac12)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Try to compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	      If no solution can be found or if
	      \begin{gather*}
		      \nabla f(x^k)^Td^k>-\rho\norm{d^k}^p\\
		      \text{\small{(\hyperref[fefb024]{insufficient decrease})}}
	      \end{gather*}
	      then fall back to $d^k:=-\nabla f(x^k)$
	\item [\textbf{(S3)}] Determine $t_k$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}
