% vim:ft=tex

\section{Nonlinear Optimization}\label{d1c9db1}

\Definition{1.2.1}{Convex sets}

A set $C\subset\R^n$ is called convex if

$$
	\lambda x+(1-\lambda)y\in C\quad
	\forall(x,y\in C,\ \lambda\in(0,1))
$$

or simply a set which contains all connecting lines of points from the
set.

\Definition{1.2.3}{Convex functions}

Let $C\subset\R^n$ be convex. Let $\lambda\in(0,1)$ and $x,y\in C$ and
let

Then $f:C\to\R$ is said to be

\begin{itemize}
	\item convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$$
	\item strictly convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$$
	\item strongly convex on $C$ if $\exists\mu>0$ such that
	      $$f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)$$
\end{itemize}

\Proposition{1.2.6}{Convexity preserving operations}

\begin{enumerate}
	\item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
	      convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
	      convex.
	\item (Composition with affine mapping) $f:\R^n\to\R$ be convex
	      and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\Definition{2.1.1}{Directional derivative}\label{37eb747}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable
at $\bar x\in\R^n$ in the direction $d\in\R^n$ if
$$
	\lim_{t\downarrow0}\frac{f(\bar x+td)-f(x)}t
$$

exists. This limit is denoted by $f'(x;d)$ and is called the
directional derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we
call it directionally differentiable.

\Lemma{2.1.2}{Directional derivative and gradient}

Let $D\subset \R^n$ be open and $f:D\to\R$
differentiable at $x\in D$. Then $f$ is directionally differentiable
at $x$ with
$$
	f'(x;d)=\nabla f(x)^Td\quad\forall(d\in\R^n)
$$

Where $f'(x;d)$ is the \hyperref[37eb747]{directional derivative} of
$f$ at $x$ in the direction $d$.

\Lemma{2.1.4}{Basic optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local
minimizer of $f$ over $X$ and $f$ is \hyperref[37eb747]{directionally
	differentiable} at $\bar x$ then
$$
	f'(x;d)\geq 0\quad\forall(d\in\R^n)
$$

\Theorem{2.1.5}{Fermat's rule}

Let $X\subset\R^n$ be open and $f:X\to\R$ differentiable at $\bar x\in
	X$. If $\bar x$ is a local minimizer (or maximizer) of $f$ over $X$
then $\nabla f(\bar x)=0$.

\Theorem{2.1.6}{Second-order necessary condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x$ is a local minimizer (maximizer) of $f$
over $X$ then $\nabla^2f(\bar x)$ is positive (negative) semidefinite.

\Lemma{2.1.7}{}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x\in\R^n$ is such that $\nabla^2f(\bar x)$
positive definite then $\exists\,\mu,\epsilon>0$ such that
$B_\epsilon(\bar x)\subset X$ and
$$
	d^T\nabla^2f(x)d\geq\mu\norm d^2_2\quad\forall(d\in\R^n,\
	x\in B_\epsilon(\bar x))
$$

\Theorem{2.1.8}{Sufficient optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. Moreover, let $\bar x$ be a stationary point of $f$
such that $\nabla^2f(\bar x)$ is positive definite. Then $\bar x$ is a
strict local minimizer of $f$.

\Definition{3.1.4}{Step-size rule}\label{ae4eac6}

Let $f:{\R}^n\to{\R}$ be continuously differentiable and
let $\mathcal A_f:=\{(x,d)\mid\nabla f(x)^Td<0\}$. A set-valued
mapping
$$
	T:(x,d)\in\mathcal A_f\mapsto T(x,d)\subset\R_{++}
$$

is called a step-size rule for $f$.

We call it well-defined for $f$ if $T(x,d)\neq0$ for all
$(x,d)\in\mathcal{A}_f$.

If the step-size rule is well-defined for all continuously
differentiable functions ${\R}^n\to{\R}$, we simply call it
well-defined.

\Definition{3.1.5}{Efficient step-size}

Let $f:\R^n\to\R$ be continuously differentiable. The
\hyperref[ae4eac6]{step-size rule} $T$ is called efficient for $f$ if
there exists $\theta>0$ such that
$$
	f(x+td)\leq f(x)-\theta\left(\frac{\nabla f(x)^Td}{\norm d}\right)^2
$$

\Definition{3.2.0}{Armijo rule and sufficient decrease}\label{fefb024}

Choose $\beta,\sigma\in(0,1)$. For $x,d\in\hyperref[ae4eac6]{\mathcal
		A_f}$ the Armijo rule $T_A$ is defined by
$$
	T_A(x,d)=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq
	f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
$$

The inequality
$$
	f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k
$$

is called the \textit{Armijo condition}. It ensures a
\textbf{sufficient decrease} on the objective function.

\Example{3.2.1}{Insufficient decrease}

Consider the function $f(x)=(x-1)^2-1$ with optimal value $f^*=-1$.

The sequence $\{x_k\}$ with $x_k:=-\frac1k$ has
$f(x_k)=\frac{1+2k}{k^2}$ and
$$
	f(x_{k+1})-f(x_k)=\frac{2k^2+4k+1}{k^2(k+1^2)}<0
$$

Hence we've found a case where the objective value decreases, but
$f(x_k)$ converges to a non-optimal value. ($f(x_k)\to0$, but we want
$f(x_k)\to-1$)

\Lemma{3.2.3}{Convergence to gradient}

Let $f:\R^n\to\R$ be continuously differentiable. Moreover, let
$\{x^k\in\R^n\}\to x$, $\{d^k\in\R^n\}\to d$ and
$\{t_k>0\}\downarrow0$. Then
$$
	\lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=\nabla f(x)^Td
$$

\begin{proof}
	By the \hyperref[c1594c9]{mean value theorem}, for all $k\in\N$,
	there exists $\eta^k\in[x^k,x^k+t_kd^k]$ such that
	$$
		f(x^k+t_kd^k)-f(x^k)=t_k\nabla f(\eta^k)^Td^k
	$$
	Clearly, $\eta^k\to x$ and hence the continuity of $\nabla f$ yields
	$$
		\nabla f(\eta^k)^Td^k\to\nabla f(x)^Td
	$$
	This readily implies
	$$
		\lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=
		\lim_{k\to\infty}\nabla f(\eta^k)^Td^k=
		\nabla f(x)^Td
	$$
\end{proof}

\Theorem{3.2.4}{Global convergence of the gradient method}\label{bbb25cd}

Let $f:\R^n\to\R$ be continuously differentiable. Then every cluster
point of a sequence generated by the \hyperref[ae01f6d]{Gradient
	method with Armijo rule} is a stationary point of $f$.

\begin{proof}
	\def\xk{\{x^k\}}
	\def\grad#1{\nabla f(#1)}
	Assume on the contrary that $\grad{\bar x}\neq0$.

	Let $\bar x$ be a cluster point of the generated sequence $\xk$, and
	let $\xk_K$ be a subsequence converging to $\bar x$. By the
	continuity of $f$, $\{f(x^k)\}\to_Kf(\bar x)$.

	As $\{f(x^k)\}$ is monotonically decreasing by the Armijo condition
	and converges on a subsequence to $f(\bar x)$, \hyperref[aaf3ba6]{by
		inspection}, $\{f(x^k)\}_\N$ converges to $f(\bar x)$.

	In particular, we have
	$$f(x^k)-f(x^{k+1})\to0$$

	Substituting $t_k=\beta^l$ and $x^{k+1}=x^k+\beta^ld^k$ into steps
	\textbf{(S2)} and \textbf{(S3)} of the algorithm, we have
	$$
		0\leq
		t_k\norm{\grad{x^k}}^2=
		-t_k\grad{x^k}^Td^k\leq
		\frac{f(x^k)-f(x^{k+1})}\sigma\to0
	$$

	Since $\grad{x^k}\to_K\grad{\bar x}\neq0$ (by continuity of $\nabla
		f$), by squeeze theorem on the above inequality, this implies that
	$t_k\to_K0$. Due to \textbf{(S3)}, for all $k\in K$ sufficiently
	large, we have
	\begin{equation*}
		f(x^k+\beta^{l_k-1}d^k)-f(x^k)>\beta^{l_k-1}\sigma\grad{x^k}^Td^k\tag*{($*$)}
	\end{equation*}

	where $\beta^{l_k}=t_k$ and $l_k\in\N$ is the exponent
	\textit{uniquely} determined by the Armijo rule in \textbf{(S3)}.
	Note that $l_k$ is the smallest value of $l$ that satisfies the
	\hyperref[fefb024]{Armijo condition}, and hence $l_k-1$ does \textit{not}
	satisfy the Armijo condition, hence $(*)$.

	Passing to the limit on $K$ an
\end{proof}

\Theorem{3.3.9}{Global convergence of \hyperref[a7a5665]{Algorithm 3.3.3}}

Let $f:\R^n\to\R$ be twice continuously differentiable. Then every
cluster point of a sequence generated by \hyperref[a7a5665]{Algorithm
	3.3.3} is a stationary point of $f$.

\begin{proof}

	\def\xk{\{x^k\}}
	\def\grad{\nabla f(x^k)}

	Let $\xk$ be generated by \hyperref[a7a5665]{Algorithm 3.3.3}. Let
	$\bar x$ be a cluster point with $\xk\to_K\bar x$. (This exists
	because $\xk$ is monotone decreasing and hence there exists a
	subsequence $\xk_K$ that converges to any given cluster point)

	If $d^k=\grad$ for infinitely $k\in K$ then the assertion follows immediately from
	% TODO: revisit this after completing Remark 3.2.5

\end{proof}

\Remark{x.x.x}{Collection of unconstrained minimization methods}

\begin{enumerate}
	\item Gradient method
	\item Globalized Newton's method
	\item Globalized BFGS method
	\item Globalized inexact Newton's method
\end{enumerate}

\Algorithm{3.1.1}{General line-search descent algorithm}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] \textit{Initialization}: Choose $x^0\in\R^n$ and put $k:=0$.
	\item [\textbf{(S1)}] \textit{Termination}: If $x^k$ satisfies a termination criterion: STOP.
	\item [\textbf{(S2)}] \textit{Search direction}: Determine $d^k$ such that $\nabla f(x^k)^Td^k<0$.
	\item [\textbf{(S3)}] \textit{Step-size}: Determine $t_k>0$ such that $f(x^k+t_kd^k)<f(x^k)$.
	\item [\textbf{(S4)}] \textit{Update}: Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.2.1}{Gradient method with Armijo rule}\label{ae01f6d}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\sigma,\beta\in(0,1)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Put $d^k:=-\nabla f(x^k)$.
	\item [\textbf{(S3)}] Determine $t_k>0$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.1}{Local Newton's method for equations}\label{abbc9be}

Goal is to solve
$$F(x)=0$$

where $F:\R^n\to\R^n$ and $F$ is assumed to be continuously differentiable.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{F(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$F'(x^k)d=-F(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.2}{Local Newton's method for unconstrained optimization}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\hyperref[abbc9be]{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	\item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{3.3.3}{Globalized Newton's method for unconstrained optimization}\label{a7a5665}

Goal is to solve
$$\min_{x\in\R^n}f(x).$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\hyperref[abbc9be]{local Newton's method for equations}.

\begin{enumerate}
	\item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\rho>0$, $p>2$, $\beta\in(0,1)$, $\sigma\in(0,\frac12)$, $\epsilon\geq0$ and put $k:=0$.
	\item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
	\item [\textbf{(S2)}] Try to compute $d^k$ as a solution of
	      $$\nabla^2f(x^k)d=-\nabla f(x^k)$$
	      If no solution can be found or if
	      \begin{gather*}
		      \nabla f(x^k)^Td^k>-\rho\norm{d^k}^p\\
		      \text{\small{(\hyperref[fefb024]{insufficient decrease})}}
	      \end{gather*}
	      then fall back to $d^k:=-\nabla f(x^k)$
	\item [\textbf{(S3)}] Determine $t_k$ by
	      $$t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}$$
	\item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Theorem{5.1.5}{}

Let $\bar x$ be a local minimum of $f\in C^1$ over $S$. Then
$$
	\nabla f(\bar x)^Td\geq0\quad\forall(d\in T_S(\bar x))
$$
% In whichever direction you choose to walk from x bar, you will
% increase the objective value

\Theorem{5.1.9}{Farkas Lemma}

Let $B\in\R^{m\times n}$, and $h\in\R^n$. The following are equivalent:
\begin{enumerate}
	\item The system $B^Tx=h$ where $x\geq0$ (element-wise) has a solution
	\item $\forall d: Bd\geq0\implies h^Td\geq0$
\end{enumerate}

\begin{proof}
	Proving that (1) implies (2).

	Let $x\geq0$ such that $B^Tx=h$.

	Then for any $d$ such that $Bd\geq0$, we have
	$$
		h^Td = (B^Tx)^Td = x^TBd
	$$

	But $x^TBd\geq0$ because $x\geq0$ and $Bd\geq0$.

	Proving that (2) implies (1) by contrapositive.

	Assume that (1) is false. Then
	$$
		h\notin\{B^Tx \mid x\geq0\}=:K
	$$

	% TODO: add this lemma
	$K$ is a closed convex cone. (closedness follows from Lemma 5.1.8)

	Set $\bar s:=P_K(h)$ and $\bar d:=\bar s-h\neq0$. (Note that $\bar
		s\in K$)

	By Proposition 5.1.7(c),
	$$
		\bar d^T(s-\bar s)\geq0\quad\forall(s\in K)
	$$

	So then with $s:=0$, we get $\bar d^T\bar s\leq0$, and with
	$s:=2\bar s$, we get $\bar d^T\bar s\geq0$ and hence
	$$
		\bar d^T\bar s=0
	$$

	And hence
	$$
		\bar d^Ts\geq0
	$$

	Then by definition of cone $K$,
	\begin{align*}
		\bar d^T B^Tx         & \geq0\quad\forall(x\geq0) \\
		\implies (B\bar d)^Tx & \geq 0
	\end{align*}


	Inserting $x:=e_i$ (where $e_i$ is the $i^\text{th}$ component
	vector) for $i=\iter1n$ implies $(B\bar d)^T\geq0$

	On the other hand
	\begin{align*}
		h^T\bar d & = (\bar s-\bar d)^T\bar d        \\
		          & = \bar s^T\bar d-\norm{\bar d}^2 \\
		          & = -\norm{\bar d}^2 \leq 0
	\end{align*}

	But since $\bar d\neq0$, we have the strict inequality $h^T\bar d<0$.

	Therefore, $B\bar d\geq0$, but $h^T\bar d<0$, i.e. (2) does not
	hold.
\end{proof}

\Definition{5.1.10}{Karush-Kuhn-Tucker conditions}\label{b38093d}

Consider the (standard) NLP:

\begin{equation*}
	\min_{x\in\R^n} f(x)\quad\text{s.t.}\quad\begin{array}{l}
		g_i(x)\leq0 \quad\forall i=\iter1m \\
		h_j(x)=0    \quad\forall j=\iter1p
	\end{array}
	\tag*{(1)}
\end{equation*}

\begin{enumerate}
	\item The function $L:\R^n\times\R^m\times\R^p\to\R$ defined by
	      $$
		      L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_ig_i(x)+\sum_{j=1}^p\mu_jh_j(x)
	      $$
	      is called the Langrangian (function) of \textbf{(1)}.
	\item The set of conditions
	      \begin{align*}
		      \nabla_xL(x,\lambda,\mu)                 & = 0 \\
		      h(x)                                     & = 0 \\
		      \lambda\geq0,\ g(x)\leq0,\ \lambda^Tg(x) & =0
	      \end{align*}
	      are called the Karush-Kuhn-Tucker conditions for \textbf{(1)},
	      where
	      $$\nabla_xL(x,\lambda,\mu)=\nabla f(x)+\sum_{i=1}^m\lambda_i\nabla g_i(x)+\sum_{j=1}^p\mu_j\nabla h_j(x)$$
	\item A triple $(\bar x,\bar\lambda,\bar\mu)$ that satisfies the
	      KKT conditions is called a KKT point.
	\item Give $\bar x$, feasible for \textbf{(1)}, we define
	      $$
		      M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of \textbf{(1)}} \}
	      $$
	      the set of all KKT multipliers at $\bar x$.
\end{enumerate}

\Definition{5.1.11}{Linearized cone}\label{ca4f471}

Let $X$ be the feasible set of \textbf{(1)}. The linearized cone (of
$X$) at $\bar x\in X$ is defined by
$$
	L_X(\bar x):=\left\{ d\in\R^n\ \middle\vert
	\begin{array}{l l}
		\nabla g_i(x)^Td\leq0 & \forall i=\iter1m \\
		\nabla h_j(x)=0       & \forall j=\iter1p
	\end{array}
	\right\}
$$

\Definition{5.1.12}{Abadie constraint qualification}\label{adc266e}

We say that the Abadie constraint qualification (ACQ) holds at $\bar
	x\in X$ if
$$
	T_X(\bar x)=L_X(\bar x)
$$

\Theorem{5.1.13}{\hyperref[b38093d]{KKT} under \hyperref[adc266e]{ACQ}}

Let $\bar x\in X$ (where $X$ is the feasible set) be a local minimum
of \textbf{(1)} such that \hyperref[adc266e]{ACQ} holds at $\bar x$.
Then there exists $(\bar\lambda,\bar\mu)\in\R^m\times\R^p$ such that
$(\bar x,\bar\lambda,\bar\mu)$ is a \hyperref[b38093d]{KKT} point of
\textbf{(1)}

\begin{proof}
	\def\bm{\bar\mu}\def\bl{\bar\lambda}\def\bx{\bar x}
	By Theorem 5.1.5,
	\begin{equation*}
		\nabla f(\bx)^Td\geq0\quad\forall(d\in T_X(\bx))\tag*{($*$)}
	\end{equation*}

	Set
	$$
		B:=\begin{pmatrix}
			-\nabla g_i(\bx)^T\quad(i=\iter1m) \\
			-\nabla h_j(\bx)^T\quad(j=\iter1p) \\
			\nabla h_j(\bx)^T\quad(j=\iter1p)
		\end{pmatrix}\in\R^{(m+2p)\times{n}}
	$$
	%TODO: check the sign of the 2nd and 3rd row

	Then $d\in L_X(\bx)\iff Bd\geq0$.

	By ACQ, we have $d\in T_X(\bx)\iff Bd\geq0$

	Combined with $(*)$, we have
	$$
		\nabla f(\bx)^Td\geq0\quad\forall(d:Bd\geq0)
	$$

	(Think $h=\nabla f(\bx)$ and apply the Farkas Lemma.)

	By the Farkas Lemma,
	$$
		\exists y=\begin{pmatrix}y^1\in\R^m\\y^2\in\R^p\\y^3\in\R^p\end{pmatrix}
	$$

	such that $y\geq0$, and $B^Ty=\nabla f(\bx)$

	Define $\bl\in\R^n,\bm\in\R^p$ by
	$$
		\bl_i=\begin{cases}
			y^1_i & \text{if $i=\iter1m$} \\
			0     & \text{otherwise}
		\end{cases}
	$$
	and
	$$
		\bm_i=\begin{cases}
			y^2_j - y^3_j & \text{if $j=\iter{m+1}{m+2p}$} \\
			0             & \text{otherwise}
		\end{cases}
	$$

	Then $(\bx,\bl,\bm)$ is a KKT point.

	MORE NOTES

	\begin{align*}
		0
		 & = \nabla f(\bx)
		+\sum_{i=0}^my^1_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}(y^2_j-y^3_j)\nabla h_j(\bx) \\
		 & = \nabla f(\bx)
		+\sum_{i=0}^m \bl_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}\bm_j\nabla h_j(\bx)
	\end{align*}

	and then there is a line with a tick/check next to it:
	$$
		\bl^Tg(\bx)=\sum_{i=0}^m\bl_ig_i(\bx)=0
	$$
\end{proof}

\Definition{}{Standard NLP}
\begin{equation*}
	\min_{x\in\R^n} f(x)\quad\text{s.t.}\quad\begin{array}{l}
		g_i(x)\leq0 \quad\forall i=\iter1m \\
		h_j(x)=0    \quad\forall j=\iter1p
	\end{array}
	\tag*{(1)}
\end{equation*}

Let $I:=\iter1m$ and $J:=\iter1p$.

We define the active set $I(\bar x)$ as
$$
	I(\bar x):=\{i\in I\mid g_i(x) = 0\}
$$

Recall:
$$
	T_X(\bar x):=\left\{
	d\in\R^n \;\middle\vert\; \exists \{x^k\in X\}\to\bar x,\{t_k\}\downarrow0:\frac{x^k-\bar x}{t_k}\to d
	\right\}
$$
This is the space where you can take a miniscule step from $\bar x$
and still remain in the feasible set. (The tangent cone)

Also recall:
$$
	L_X(\bar x):=\left\{
	d\in\R^n
	\;\middle\vert\;
	\begin{array}{l l}
		g_i(\bar x)\leq0 & \forall i\in I(\bar x) \\
		h_j(\bar x)=0    & \forall j\in J
	\end{array}
	\right\}
$$
This is the linearized cone.

\Definition{5.1.14}{Constraint qualifications}

A condition of $X$ (i.e. on $g$ and $h$) that ensures that the KKT
conditions hold at a local minimizer is called a constraint
qualification.

\Definition{5.1.15}{}

Let $\bar x$ be feasible for (1). We say that

\begin{enumerate}\renewcommand{\theenumi}{\alph{enumi}}
	\item the linear independence constraint qualification (LICQ) holds
	      at $\bar x$ if
	      $$
		      \nabla g_i(\bar x)\quad\forall(i\in I(\bar x)),\ \nabla h_j(\bar x)\quad\forall(j\in J)
	      $$
	      are linearly indepdendent
	\item the Mangasarian-Fromovitz constraint qualification (MFCQ)
	      holds at $\bar x$ if
	      $$
		      \nabla h_j(\bar x)\quad\forall(j\in J)
	      $$
	      are linearly indepdendent, and
	      \begin{gather*}
		      \nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
		      \nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
	      \end{gather*}
\end{enumerate}

\Proposition{5.1.16}{LICQ $\implies$ MFCQ}

Let $\bar x$ be feasible for (1) such that LICQ holds at $\bar x$.
Then $MFCQ$ holds.

\Lemma{5.1.17}{}

Let $\bar x$ be feasible for (1) such that there exists $d\in\R^n$
such that
$$
	\nabla h_j(\bar x)\quad\forall(j\in J)
$$
are linearly indepdendent, and
\begin{gather*}
	\nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
	\nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
\end{gather*}

Then there exists $\epsilon>0$ and a $C^1$-curve
$\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
\begin{gather*}
	\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
	\gamma(0) = \bar x \\
	\gamma'(0)=d
\end{gather*}


\Lemma{5.1.18}{}

Let $\bar x$ be feasible for (1). Then
$$
	T_X(\bar x)\subset L_X(\bar x)
$$

\begin{proof}
	Exercise 9.3
\end{proof}

\Proposition{5.1.19}{MFCQ $\implies$ ACQ}

Let $\bar x$ be feasible for (1) such that MFCQ holds at $\bar x$.
Then ACQ holds at $\bar x$.

\begin{proof}
	Only need to show $L_X(\bar x)\subset T_X(\bar x)$.

	Let $d\in L_X(\bar x)$. By MFCQ, there exists $\hat d$ such that
	% Note: we can't use d directly instead of d(∂) because d doesn't
	% have the strict inequality required by Lemma 5.1.17
	\begin{gather*}
		\nabla g_i(\bar x)^T\hat d<0\quad\forall(i\in I(\bar x)) \\
		\nabla h_j(\bar x)^T\hat d=0\quad\forall(j\in J)
	\end{gather*}
	and $\nabla h_j(\bar x)\ \forall(j\in J)$ are linearly indepdendent.

	Set $d(\delta):= d + \delta\hat d$. Then, $\forall\delta>0$:
	\begin{gather*}
		\nabla g_i(\bar x)^T{d(\delta)}<0\quad\forall(i\in I(\bar x)) \\
		\nabla h_j(\bar x)^T{d(\delta)}=0\quad\forall(j\in J)
	\end{gather*}

	Applying Lemma 5.1.17 to $d(\delta)$ yields a $C^1$-curve
	$\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
	\begin{gather*}
		\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
		\gamma(0) = \bar x \\
		\gamma'(0)=d(\delta)
	\end{gather*}

	Let $\{t_k\}\downarrow0$ and set $x^k:=\gamma(t_k)$. Then
	$\{x^k\in X\}\to\bar x$.

	And $d(\delta)=\gamma'(0)$ and hence
	$$
		d(\delta)=\lim_{k\to\infty}\frac{\gamma(t_k)-\gamma(0)}{t_k}=
		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}
	$$

	And since
	$$
		\lim_{\delta\to0} d(\delta) = d
	$$
	and both lie in $T_X(\bar x)$ due to an unproved argument using
	closedness.

	this completes the proof with
	$$
		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}=d
	$$
\end{proof}

\Corollary{3.1.20}{KKT under MFCQ}

Let $\bar x$ be a local minimum of (1) such that MFCQ holds at $\bar x$. Then:

\begin{enumerate}\renewcommand{\theenumi}{\alph{enumi}}
	\item There exists $(\bar\lambda, \bar\mu)\in\R^m\times\R^p$  such
	      that $(\bar x,\bar\lambda, \bar\mu)$ is a KKT point of (1).
	\item $M(\bar x)$ is bounded where
	      $$
		      M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of \textbf{(1)}} \}
	      $$
\end{enumerate}

\begin{proof}
	Proving (a) requires Proposition 5.1.19 (MFCQ $\implies$ ACQ) and
	Theorem 5.1.13

	Proving (b):

	Suppose $M(\bar x)$ were unbounded, i.e. there exists
	$$
		\{(\lambda^k,\mu^k)\in M(\bar x)\} : \norm{(\lambda^k,\mu^k)}\to+\infty
	$$

	Then WLOG,
	$$
		\frac{(\lambda^k,\mu^k)}{\norm{(\lambda^k,\mu^k)}}\to(\tilde\lambda,\tilde\mu)
	$$

	Note that every element in the above sequence has norm 1. And hence
	we know that it doesn't converge to the zero vector, since that has
	norm 0.

	Since $(\bar x,\lambda^k,\mu^k)$ is a KKT point of (1), we have
	$$
		0 = \frac{\nabla f(\bar x) + \sum_{i\in I(\bar x)}\lambda^k_i\nabla g_i(\bar x)
			+ \sum_{j\in J}\mu^k_j\nabla h_j(\bar x)}{\norm{(\lambda^k,\mu^k)}}
	$$

	Then as $h\to\infty$,

	\begin{equation*}
		0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)
		+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)
	\end{equation*} % cognitive, but just raw algebra

	% Note: taking ||.|| of a product space (X × Y)
	% ||(a,b)|| = ||a||+||b||
\end{proof}

Now multiply with $d$ from MFCQ at $\bar x$.
\begin{equation*}
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
	+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td \tag*{($*$)}
\end{equation*}

And by MFCQ, the second term is $=0$. Hence
$$
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
$$

\textbf{Case 1} $\exists i_0 \in I(\bar x): \tilde\lambda_{i_0}>0$. Then
$0<0$. Contradiction!
$$
	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
	\;\leq\;\tilde\lambda_{i_0}\nabla g_{i_0}(\bar x)^Td<0
$$
\textbf{Case 2} $\forall i\in I(\bar x):\tilde\lambda_i=0$. Then ($*$) reads
$$
	0=\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td
$$
which is a contradiction against MFCQ.

\Corollary{5.1.21}{}

Let $\bar x$ be a local minimum of (1) such that LICQ holds at $\bar
	x$. Then:
\begin{enumerate}\renewcommand{\theenumi}{\alph{enumi}}
	\item There exists $(\bar\lambda,\bar\mu)\in M(\bar x)$.
	\item $M(\bar x) = \{(\bar\lambda,\bar\mu)\}$.
\end{enumerate}

\begin{proof}
	(a) follows from Proposition 5.1.16 + Corollary 5.1.19

	(b)

	Assume that $(\tilde\lambda,\tilde\mu)\in M(\bar x)$. Then
	\begin{align*}
		0          & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)                             \\
		           & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\bar\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\bar\mu_j\nabla h_j(\bar x)                                 \\
		\implies 0 & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}(\tilde\lambda_i-\bar\lambda_i)\nabla g_i(\bar x)+\sum_{j\in J}(\tilde\mu_j-\bar\mu_j)\nabla h_j(\bar x)
	\end{align*}

	Then by LICQ,
	\begin{gather*}
		\tilde\lambda_i - \bar\lambda_i = 0\quad\forall(i\in I(\bar x)) \\
		\tilde\mu_j - \bar\mu_j = 0\quad\forall(j\in J) \\
	\end{gather*}

	Then since $\tilde\lambda_i = \bar\lambda_i\ \forall(i\notin I(\bar
		x))$, this shows $\tilde\lambda=\bar\lambda$ and $\tilde\mu=\bar\mu$.
\end{proof}




% 		     
% $$
% are linearly indepdendent, and
% \begin{gather*}
% 		      \nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
% 		      \nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
% Proving that MFCQ implies LICQ is hard.
