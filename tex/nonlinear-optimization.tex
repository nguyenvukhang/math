% vim:ft=tex

\section{Nonlinear Optimization}\label{d1c9db1}

\Definition{1.2.1}{Convex sets}

A set $C\subset\R^n$ is called convex if

$$
	\lambda x+(1-\lambda)y\in C\quad
	\forall(x,y\in C,\ \lambda\in(0,1))
$$

or simply a set which contains all connecting lines of points from the
set.

\Definition{1.2.3}{Convex functions}

Let $C\subset\R^n$ be convex. Let $\lambda\in(0,1)$ and $x,y\in C$ and
let

Then $f:C\to\R$ is said to be

\begin{itemize}
	\item convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$$
	\item strictly convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$$
	\item strongly convex on $C$ if $\exists\mu>0$ such that
	      $$f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)$$
\end{itemize}

\Proposition{1.2.6}{Convexity preserving operations}

\begin{enumerate}
	\item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
	      convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
	      convex.
	\item (Composition with affine mapping) $f:\R^n\to\R$ be convex
	      and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\Definition{2.1.1}{Directional derivative}\label{37eb747}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable
at $\bar x\in\R^n$ in the direction $d\in\R^n$ if
$$
	\lim_{t\downarrow0}\frac{f(\bar x+td)-f(x)}t
$$

exists. This limit is denoted by $f'(x;d)$ and is called the
directional derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we
call it directionally differentiable.

\Lemma{2.1.2}{Directional derivative and gradient}

Let $D\subset \R^n$ be open and $f:D\to\R$
differentiable at $x\in D$. Then $f$ is directionally differentiable
at $x$ with
$$
	f'(x;d)=\nabla f(x)^Td\quad\forall(d\in\R^n)
$$

Where $f'(x;d)$ is the \hyperref[37eb747]{directional derivative} of
$f$ at $x$ in the direction $d$.

\Lemma{2.1.4}{Basic optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local
minimizer of $f$ over $X$ and $f$ is \hyperref[37eb747]{directionally
	differentiable} at $\bar x$ then
$$
	f'(x;d)\geq 0\quad\forall(d\in\R^n)
$$

\Theorem{2.1.5}{Fermat's rule}

Let $X\subset\R^n$ be open and $f:X\to\R$ differentiable at $\bar x\in
	X$. If $\bar x$ is a local minimizer (or maximizer) of $f$ over $X$
then $\nabla f(\bar x)=0$.

\Theorem{2.1.6}{Second-order necessary condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x$ is a local minimizer (maximizer) of $f$
over $X$ then $\nabla^2f(\bar x)$ is positive (negative) semidefinite.

\Lemma{2.1.7}{}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. If $\bar x\in\R^n$ is such that $\nabla^2f(\bar x)$
positive definite then $\exists\,\mu,\epsilon>0$ such that
$B_\epsilon(\bar x)\subset X$ and
$$
	d^T\nabla^2f(x)d\geq\mu\norm d^2_2\quad\forall(d\in\R^n,\
	x\in B_\epsilon(\bar x))
$$

\Theorem{2.1.8}{Sufficient optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously
differentiable. Moreover, let $\bar x$ be a stationary point of $f$
such that $\nabla^2f(\bar x)$ is positive definite. Then $\bar x$ is a
strict local minimizer of $f$.

\Definition{3.1.4}{Step-size rule}\label{ae4eac6}

Let $f:{\R}^n\to{\R}$ be continuously differentiable and
let $\mathcal A_f:=\{(x,d)\mid\nabla f(x)^Td<0\}$. A set-valued
mapping
$$
	T:(x,d)\in\mathcal A_f\mapsto T(x,d)\subset\R_{++}
$$

is called a step-size rule for $f$.

We call it well-defined for $f$ if $T(x,d)\neq0$ for all
$(x,d)\in\mathcal{A}_f$.

If the step-size rule is well-defined for all continuously
differentiable functions ${\R}^n\to{\R}$, we simply call it
well-defined.

\Definition{3.1.5}{Efficient step-size}

Let $f:\R^n\to\R$ be continuously differentiable. The
\hyperref[ae4eac6]{step-size rule} $T$ is called efficient for $f$ if
there exists $\theta>0$ such that
$$
	f(x+td)\leq f(x)-\theta\left(\frac{\nabla f(x)^Td}{\norm d}\right)^2
$$
