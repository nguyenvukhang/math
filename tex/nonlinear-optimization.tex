% vim:ft=tex

\section{Nonlinear Optimization}\label{d1c9db1}

\Definition{1.2.1}{Convex sets}

A set $C\subset\R^n$ is called convex if

$$
	\lambda x+(1-\lambda)y\in C\quad
	\forall(x,y\in C,\ \lambda \in(0,1))
$$

or simply a set which contains all connecting lines of points from the
set.

\Definition{1.2.3}{Convex functions}

Let $C\subset\R^n$ be convex. Let $\lambda\in(0,1)$ and $x,y\in C$ and
let

Then $f:C\to\R$ is said to be

\begin{itemize}
	\item convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)$$
	\item strictly convex on $C$ if
	      $$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$$
	\item strongly convex on $C$ if $\exists\mu>0$ such that
	      $$f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)$$
\end{itemize}

\Proposition{1.2.6}{Convexity preserving operations}

\begin{enumerate}
	\item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
	      convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
	      convex.
	\item (Composition with affine mapping) $f:\R^n\to\R$ be convex
	      and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\paragraph{2.1.1}\label{37eb747}{Directional derivative}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable
at $\bar x\in\R^n$ in the direction $d\in\R^n$ if
$$
	\lim_{t\downarrow0}\frac{f(\bar x+td)-f(x)}t
$$

exists. This limit is denoted by $f'(x;d)$ and is called the
directional derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we
call it directionally differentiable.

\Lemma{2.1.2}{Directional derivative and gradient}

Let $D\subset \R^n$ be open and $f:D\to\R$
differentiable at $x\in D$. Then $f$ is directionally differentiable
at $x$ with
$$
	f'(x;d)=\nabla f(x)^Td\quad\forall(d\in\R^n)
$$

Where $f'(x;d)$ is the directional derivative of $f$ at $x$ in the
direction $d$.

\Lemma{2.1.4}{Basic optimality condition}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local
minimizer of $f$ over $X$ and $f$ is \hyperref[37eb747]{directionally
differentiable} at $\bar x$ then
$$
	f'(x;d)\geq 0\quad\forall(d\in\R^n)
$$
