\section{Nonlinear Optimization, Part II: Constrained Optimization}\label{d1c9db1}

\def\pd{\succ} % positive definite
\def\psd{\succeq} % positive semidefinite
\def\NlpStdForm{\begin{array}{l l l l}
		\displaystyle \min_{x\in\R^n}f(x)
		 & \text{s.t.} & g_i(x)\leq0 & \forall i\in I \\
		 &             & h_j(x)=0    & \forall j\in J
	\end{array}}

\Definition{5.0.0}{Standard Nonlinear Program}\label{bbe9993}

\begin{equation*}
	\NlpStdForm\Tag{*}
\end{equation*}
Where $f,g_i,h_j:\R^n\to\R$ are continuously differentiable.

We call $(*)$ a nonlinear program (NLP) in standard form.

By convention, we let the feasible set of $(*)$ be denoted by $X$, with
\begin{equation*}
	X:=\Set{x\in\R^n}{\begin{array}{l l}
			g_i(x)\leq0 & \forall i\in I \\
			h_j(x)=0    & \forall j\in J
		\end{array}\Tag{**}}
\end{equation*}

By the continuity of the constraint functions, $X$ is closed.

We will use $I:=\{\iter1m\}$ and $J:=\{\iter1p\}$, and define the
\textbf{active set} at $\bar x\in X$ as
$$
	I(\bar x):=\Set{i\in I}{g_i(\bar x)=0}
$$

\Definition{5.1.1}{Cones}

A non-empty set $K\subset\R^n$ is said to be a cone if
$$
	\lambda v\in K\with(\lambda\geq0,\ v\in K)
$$

i.e. $K$ is a cone if and only if it is closed under multiplication
with non-negative scalars.

\Example{5.1.2}{Examples of cones}

\begin{enumerata}
	\item \textit{(Non-negative Orthant)} For all $n\in\N$, the
	non-negative orthan $\R^n_+$ is a convex cone, which is also a polyhedron as
	$$
		\R^n_+=\Set{x\in\R^n}{(-e_i)^Tx\leq 0,\ \forall i=\iter1n}
	$$
	\item \textit{(Cone complimentary constraints)} Let $K\subset \R^n$ be
	a cone. Then the set
	$$
		\Lambda:=\Set{(x,y)\in\R^n\times\R^n}{x,y\in K,\ \inner xy=0}
	$$
	is a cone. A prominent example is $K=\R^n$, in which case $\Lambda$
	is called the complementary constraint set.
	\item \textit{(Positive semidefinite matrices)} For $n\in\N$, the set
	of positive semidefinite $n\times n$ matrices is a convex cone in
	the space of $n\times n$ symmetric matrices.
\end{enumerata}

\Definition{5.1.3}{Tangent cone}\label{add7a4b}

Let $S\subset\R^n$ and $\bar x\in S$. Then the set
$$
	T_S(\bar x):=\Set{d\in\R^n}
	{\exists\{x^k\in S\}\to\bar x,\{t_k\}\downarrow0:\frac{x^k-\bar x}{t_k}\to d}
$$
is called the (Bouligand) tangent cone of $S$ at $\bar x$.

\Proposition{5.1.4}{}

Let $S\subset\R^n$ and $x\in S$. Then $T_S(x)$ is a closed cone.

\Theorem{5.1.5}{Basic first-order optimality condition}\label{c8e5836}

Let $\bar x$ be a local minimizer of $f\in C^1$ over $S$. Then the following hold:
\begin{enumerata}
	\item $\nabla f(\bar x)^Td\geq0\quad(d\in T_S(\bar x))$
	\item If $S$ is convex then
	$$
		\nabla f(\bar x)^T(x-\bar x)\geq 0\with(x\in S)
	$$
\end{enumerata}

\Definition{5.1.6}{Projection on a set}

Let $S\subset\R^n$ be non-empty and $x\in\R^n$. Then we define the
projection of $x$ on $S$ by
$$
	P_S(x):=\argmin_{y\in S}\norm{x-y}
$$

\Proposition{5.1.7}{Projection on a closed convex set}\label{ce30ae7}

Let $x\in\R^n$ and $S\subset\R^n$ be a non-empty, closed, and convex
set. Then the following hold:
\begin{enumerata}
	\item $P_S(x)$ has exactly one element, i.e. $P$ is a function
	$\R^n\to S$
	\item $P_S(x)=x$ if and only if $x\in S$
	\item $(P_S(x)-x)^T(y-P_S(x))\geq0\with(y\in S)$
\end{enumerata}

\begin{proof}
	(a) follows immediately from \href{f546fc9}{Theorem 2.2.7(d)} since
	the function $y\mapsto\frac12\norm{x-y}^2$ is strongly convex.

	(b) is obvious.

	(c) follows from \href{c8e5836}{Theorem 5.1.5(b)} applied to
	$f:y\mapsto\frac12\norm{x-y}^2$.
\end{proof}

\Lemma{5.1.8}{}\label{d2dff14}

Let $B\in\R^{l\times n}$. Then
$$
	K:=\Set{B^Tx}{x\geq0}
$$

is a (non-empty) closed, convex cone.

\Theorem{5.1.9}{Farkas' Lemma}\label{d64b0db}

Let $B\in\R^{l\times n}$ and $h\in\R^n$. Then the system
$$
	B^Tx=h\with(x\in\R^l,\ x\geq0)
$$

has a solution if and only if $h^Td\geq0$ for all $d\in\R^n$ such that
$Bd\geq0$.

\begin{proof}
	Proving that (1) implies (2):

	Let $x\geq0$ such that $B^Tx=h$. Then for any $d$ such that
	$Bd\geq0$, we have
	$$
		h^Td=(B^Tx)^Td=x^TBd
	$$

	But $x^TBd\geq0$ because $x\geq0$ and $Bd\geq0$.

	Proving that (2) implies (1) by contrapositive.

	Assume that (1) is false. Then
	$$
		h\notin\{B^Tx \mid x\geq0\}=:K
	$$

	By \href{d2dff14}{Lemma 5.1.8}, $K$ is a closed convex cone.

	Set $\bar s:=P_K(h)$ and $\bar d:=\bar s-h$. Note that $\bar
		s\in K$, and $h\notin K$, and hence $\bar d\neq0$.

	By \href{ce30ae7}{Proposition 5.1.7(c)},
	\begin{equation*}
		\bar d^T(s-\bar s)\geq0\quad\forall(s\in K)\Tag{*}
	\end{equation*}

	Substituting $s:=0$ and $s:=2\bar s$, we obtain two simultaneous
	inequalities
	\begin{align*}
		\bar d^T\bar s\leq0\Quad\text{and}\Quad\bar d^T\bar s\geq0
	\end{align*}

	And hence $\bar d^T\bar s=0$. Using this with $(*)$ gives
	$$
		\bar d^Ts\geq0
	$$

	Then by definition of cone $K$, for all $x\geq0$,
	\begin{align*}
		\bar d^T B^Tx     & \geq0 \\
		\then(B\bar d)^Tx & \geq0
	\end{align*}

	Inserting $x:=e_i$ (where $e_i$ is the $i^\text{th}$ component
	vector) for $i=\iter1n$ implies $(B\bar d)^T\geq0$.

	On the other hand (recall $\bar d^T\bar s=0$ from above)
	\begin{align*}
		h^T\bar d & =(\bar s-\bar d)^T\bar d        \\
		          & =\bar s^T\bar d-\norm{\bar d}^2 \\
		          & =-\norm{\bar d}^2               \\
		          & \leq 0
	\end{align*}

	But since $\bar d\neq0$, we have the strict inequality $h^T\bar
		d<0$.

	Therefore, $B\bar d\geq0$, but $h^T\bar d<0$, i.e. (2) does not
	hold.
\end{proof}

\Definition{5.1.10}{Karush-Kuhn-Tucker conditions}\label{b38093d}

Consider the (standard) NLP:

\begin{equation*}
	\NlpStdForm
	\Tag{1}
\end{equation*}
where we let $X$ be the feasible set of (1).

\begin{enumerate}
	\item The function $L:\R^n\times\R^m\times\R^p\to\R$ defined by
	      $$
		      L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_ig_i(x)+\sum_{j=1}^p\mu_jh_j(x)
	      $$
	      is called the Langrangian (function) of (1).
	\item The set of conditions
	      $$
		      \begin{array}{l l}
			      \nabla_xL(x,\lambda,\mu)                 & =0, \\[0.2em]
			      h(x)                                     & =0, \\[0.2em]
			      \lambda\geq0,\ g(x)\leq0,\ \lambda^Tg(x) & =0
		      \end{array}
	      $$
	      are called the Karush-Kuhn-Tucker conditions for (1), where
	      $$
		      \nabla_xL(x,\lambda,\mu)=\nabla f(x)+\sum_{i=1}^m\lambda_i\nabla g_i(x)+\sum_{j=1}^p\mu_j\nabla h_j(x)
	      $$
	\item A triple $(\bar x,\bar\lambda,\bar\mu)$ that satisfies the
	      KKT conditions is called a KKT point.
	\item Given $\bar x$, a feasible point for (1), we define
	      $$
		      M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of (1)}\}
	      $$
	      as the set of all KKT multipliers (possibly empty) at $\bar x$.
\end{enumerate}

\Definition{5.1.11}{Linearized cone}\label{ca4f471}

Let $X$ be the feasible set of (1). The linearized cone (of $X$) at
$\bar x\in X$ is defined by
$$
	L_X(\bar x):=\left\{ d\in\R^n\ \middle\vert
	\begin{array}{l l}
		\nabla g_i(\bar x)^Td\leq0 & \forall i=I(\bar x) \\
		\nabla h_j(\bar x)^Td=0    & \forall j=\iter1p
	\end{array}
	\right\}
$$

\Definition{5.1.12}{Abadie constraint qualification (ACQ)}\label{adc266e}

We say that the ACQ holds at $\bar x\in X$ if
$$
	T_X(\bar x)=L_X(\bar x)
$$

That is, the \href{add7a4b}{tangent cone} is exactly the
\href{ca4f471}{linearized cone}.

\Theorem{5.1.13}{\href{b38093d}{KKT} conditions under \href{adc266e}{ACQ}}\label{b1c5437}

Let $\bar x\in X$ be a local minimizer of (1) such that
\href{adc266e}{ACQ} holds at $\bar x$. Then there exists
$(\bar\lambda,\bar\mu)\in\R^m\times\R^p$ such that $(\bar
	x,\bar\lambda,\bar\mu)$ is a \href{b38093d}{KKT} point of (1).

\begin{proof}
	\def\bm{\bar\mu}\def\bl{\bar\lambda}\def\bx{\bar x}
	By Theorem 5.1.5,
	\begin{equation*}
		\nabla f(\bx)^Td\geq0\quad\forall(d\in T_X(\bx))\tag*{($*$)}
	\end{equation*}

	Set
	$$
		B:=\begin{pmatrix}
			-\nabla g_i(\bx)^T\quad(i=\iter1m) \\
			-\nabla h_j(\bx)^T\quad(j=\iter1p) \\
			\nabla h_j(\bx)^T\quad(j=\iter1p)
		\end{pmatrix}\in\R^{(m+2p)\times{n}}
	$$
	%TODO: check the sign of the 2nd and 3rd row

	Then $d\in L_X(\bx)\iff Bd\geq0$.

	By ACQ, we have $d\in T_X(\bx)\iff Bd\geq0$

	Combined with $(*)$, we have
	$$
		\nabla f(\bx)^Td\geq0\quad\forall(d:Bd\geq0)
	$$

	(Think $h=\nabla f(\bx)$ and apply the Farkas Lemma.)

	By the Farkas Lemma,
	$$
		\exists y=\begin{pmatrix}y^1\in\R^m\\y^2\in\R^p\\y^3\in\R^p\end{pmatrix}
	$$

	such that $y\geq0$, and $B^Ty=\nabla f(\bx)$

	Define $\bl\in\R^n,\bm\in\R^p$ by
	$$
		\bl_i=\begin{cases}
			y^1_i & \text{if $i=\iter1m$} \\
			0     & \text{otherwise}
		\end{cases}
	$$
	and
	$$
		\bm_i=\begin{cases}
			y^2_j - y^3_j & \text{if $j=\iter{m+1}{m+2p}$} \\
			0             & \text{otherwise}
		\end{cases}
	$$

	Then $(\bx,\bl,\bm)$ is a KKT point.

	MORE NOTES

	\begin{align*}
		0
		 & = \nabla f(\bx)
		+\sum_{i=0}^my^1_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}(y^2_j-y^3_j)\nabla h_j(\bx) \\
		 & = \nabla f(\bx)
		+\sum_{i=0}^m \bl_i\nabla g_i(\bx)
		+\sum_{j=m+1}^{m+2p}\bm_j\nabla h_j(\bx)
	\end{align*}

	and then there is a line with a tick/check next to it:
	$$
		\bl^Tg(\bx)=\sum_{i=0}^m\bl_ig_i(\bx)=0
	$$
\end{proof}

\Definition{5.1.14}{Constraint qualifications}\label{e8fa554}

A condition on $X$ (i.e. on $g$ and $h$) that ensures that the KKT
conditions hold at a local minimizer is called a \textbf{constraint
	qualification}.


\Definition{5.1.15}{LICQ and MFCQ}\label{fed784a}

Let $\bar x$ be feasible for (1). We say that
\begin{enumerata}
	\item \textbf{(LICQ)} the linear independence constraint
	qualification holds at $\bar x$ if the gradients
	\begin{align*}
		\nabla g_i(\bar x) & \quad(i\in I(\bar x)), \\
		\nabla h_j(\bar x) & \quad(j\in J)
	\end{align*}
	are linearly indepdendent.
	\item \textbf{(MFCQ)} the Mangasarian-Fromovitz constraint
	qualification holds at $\bar x$ if the gradients
	$$
		\nabla h_j(\bar x)\quad(j\in J)
	$$
	are linearly indepdendent, and $\exists d\in\R^n$ such that
	\begin{align*}
		\nabla g_i(\bar x)^Td & <0\quad(i\in I(\bar x)) \\
		\nabla h_j(\bar x)^Td & =0\quad(j\in J)
	\end{align*}
\end{enumerata}

\Proposition{5.1.16}{LICQ implies MFCQ}\label{a7ef3f5}

Let $\bar x$ be feasible for (1) such that \href{fed784a}{LICQ} holds
at $\bar x$. Then \href{fed784a}{MFCQ} holds.

% \Lemma{5.1.17}{}\label{a9bea04}

% Let $\bar x$ be feasible for (1) such that there exists $d\in\R^n$
% such that
% $$
% 	\nabla h_j(\bar x)\quad\forall(j\in J)
% $$
% are linearly indepdendent, and
% \begin{gather*}
% 	\nabla g_i(\bar x)^T < 0  \quad\forall(i\in I(\bar x))\\
% 	\nabla h_j(\bar x)^Td = 0 \quad\forall(j\in J)
% \end{gather*}

% Then there exists $\epsilon>0$ and a $C^1$-curve
% $\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
% \begin{gather*}
% 	\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
% 	\gamma(0) = \bar x \\
% 	\gamma'(0)=d
% \end{gather*}

% \Lemma{5.1.18}{}\label{a08cd35}

% Let $\bar x$ be feasible for (1). Then
% $$
% 	T_X(\bar x)\subset L_X(\bar x)
% $$

% \begin{proof}
% 	Exercise 9.3
% \end{proof}

% \Proposition{5.1.19}{MFCQ $\implies$ ACQ}\label{eddce03}

% Let $\bar x$ be feasible for (1) such that MFCQ holds at $\bar x$.
% Then ACQ holds at $\bar x$.

% \begin{proof}
% 	Only need to show $L_X(\bar x)\subset T_X(\bar x)$.

% 	Let $d\in L_X(\bar x)$. By MFCQ, there exists $\hat d$ such that
% 	% Note: we can't use d directly instead of d(∂) because d doesn't
% 	% have the strict inequality required by Lemma 5.1.17
% 	\begin{gather*}
% 		\nabla g_i(\bar x)^T\hat d<0\quad\forall(i\in I(\bar x)) \\
% 		\nabla h_j(\bar x)^T\hat d=0\quad\forall(j\in J)
% 	\end{gather*}
% 	and $\nabla h_j(\bar x)\ \forall(j\in J)$ are linearly indepdendent.

% 	Set $d(\delta):= d + \delta\hat d$. Then, $\forall\delta>0$:
% 	\begin{gather*}
% 		\nabla g_i(\bar x)^T{d(\delta)}<0\quad\forall(i\in I(\bar x)) \\
% 		\nabla h_j(\bar x)^T{d(\delta)}=0\quad\forall(j\in J)
% 	\end{gather*}

% 	Applying Lemma 5.1.17 to $d(\delta)$ yields a $C^1$-curve
% 	$\gamma:(-\epsilon,\epsilon)\to\R^n$ such that
% 	\begin{gather*}
% 		\gamma(t) \in X\quad\forall(t\in(-\epsilon,\epsilon)) \\
% 		\gamma(0) = \bar x \\
% 		\gamma'(0)=d(\delta)
% 	\end{gather*}

% 	Let $\{t_k\}\downarrow0$ and set $x^k:=\gamma(t_k)$. Then
% 	$\{x^k\in X\}\to\bar x$.

% 	And $d(\delta)=\gamma'(0)$ and hence
% 	$$
% 		d(\delta)=\lim_{k\to\infty}\frac{\gamma(t_k)-\gamma(0)}{t_k}=
% 		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}
% 	$$

% 	And since
% 	$$
% 		\lim_{\delta\to0} d(\delta) = d
% 	$$
% 	and both lie in $T_X(\bar x)$ due to an unproved argument using
% 	closedness.

% 	this completes the proof with
% 	$$
% 		\lim_{k\to\infty}\frac{x^k-\bar x}{t_k}=d
% 	$$
% \end{proof}

% \Corollary{3.1.20}{KKT under MFCQ}\label{b39f0fe}

% Let $\bar x$ be a local minimum of (1) such that MFCQ holds at $\bar x$. Then:

% \begin{enumerata}
% 	\item There exists $(\bar\lambda, \bar\mu)\in\R^m\times\R^p$  such
% 	that $(\bar x,\bar\lambda, \bar\mu)$ is a KKT point of (1).
% 	\item $M(\bar x)$ is bounded where
% 	$$
% 		M(\bar x):=\{(\lambda,\mu)\mid(\bar x,\lambda,\mu)\text{ is a KKT point of (1)} \}
% 	$$
% \end{enumerata}

% \begin{proof}
% 	Proving (a) requires Proposition 5.1.19 (MFCQ $\implies$ ACQ) and
% 	Theorem 5.1.13

% 	Proving (b):

% 	Suppose $M(\bar x)$ were unbounded, i.e. there exists
% 	$$
% 		\{(\lambda^k,\mu^k)\in M(\bar x)\} : \norm{(\lambda^k,\mu^k)}\to+\infty
% 	$$

% 	Then WLOG,
% 	$$
% 		\frac{(\lambda^k,\mu^k)}{\norm{(\lambda^k,\mu^k)}}\to(\tilde\lambda,\tilde\mu)
% 	$$

% 	Note that every element in the above sequence has norm 1. And hence
% 	we know that it doesn't converge to the zero vector, since that has
% 	norm 0.

% 	Since $(\bar x,\lambda^k,\mu^k)$ is a KKT point of (1), we have
% 	$$
% 		0 = \frac{\nabla f(\bar x) + \sum_{i\in I(\bar x)}\lambda^k_i\nabla g_i(\bar x)
% 			+ \sum_{j\in J}\mu^k_j\nabla h_j(\bar x)}{\norm{(\lambda^k,\mu^k)}}
% 	$$

% 	Then as $h\to\infty$,

% 	\begin{equation*}
% 		0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)
% 		+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)
% 	\end{equation*} % cognitive, but just raw algebra

% 	% Note: taking ||.|| of a product space (X × Y)
% 	% ||(a,b)|| = ||a||+||b||
% \end{proof}

% Now multiply with $d$ from MFCQ at $\bar x$.
% \begin{equation*}
% 	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
% 	+ \sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td \tag*{($*$)}
% \end{equation*}

% And by MFCQ, the second term is $=0$. Hence
% $$
% 	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
% $$

% \textbf{Case 1} $\exists i_0 \in I(\bar x): \tilde\lambda_{i_0}>0$. Then
% $0<0$. Contradiction!
% $$
% 	0=\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)^Td
% 	\;\leq\;\tilde\lambda_{i_0}\nabla g_{i_0}(\bar x)^Td<0
% $$
% \textbf{Case 2} $\forall i\in I(\bar x):\tilde\lambda_i=0$. Then ($*$) reads
% $$
% 	0=\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)^Td
% $$
% which is a contradiction against MFCQ.

% \Corollary{5.1.21}{}\label{c55a5f3}

% Let $\bar x$ be a local minimum of (1) such that LICQ holds at $\bar
% 	x$. Then:
% \begin{enumerata}
% 	\item There exists $(\bar\lambda,\bar\mu)\in M(\bar x)$.
% 	\item $M(\bar x) = \{(\bar\lambda,\bar\mu)\}$.
% \end{enumerata}

% \begin{proof}
% 	(a) follows from Proposition 5.1.16 + Corollary 5.1.19

% 	(b)

% 	Assume that $(\tilde\lambda,\tilde\mu)\in M(\bar x)$. Then
% 	\begin{align*}
% 		0          & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\tilde\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\tilde\mu_j\nabla h_j(\bar x)                             \\
% 		           & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}\bar\lambda_i\nabla g_i(\bar x)+\sum_{j\in J}\bar\mu_j\nabla h_j(\bar x)                                 \\
% 		\implies 0 & =\nabla f(\bar x)+\sum_{i\in I(\bar x)}(\tilde\lambda_i-\bar\lambda_i)\nabla g_i(\bar x)+\sum_{j\in J}(\tilde\mu_j-\bar\mu_j)\nabla h_j(\bar x)
% 	\end{align*}

% 	Then by LICQ,
% 	\begin{gather*}
% 		\tilde\lambda_i - \bar\lambda_i = 0\quad\forall(i\in I(\bar x)) \\
% 		\tilde\mu_j - \bar\mu_j = 0\quad\forall(j\in J) \\
% 	\end{gather*}

% 	Then since $\tilde\lambda_i = \bar\lambda_i\ \forall(i\notin I(\bar
% 		x))$, this shows $\tilde\lambda=\bar\lambda$ and $\tilde\mu=\bar\mu$.
% \end{proof}
